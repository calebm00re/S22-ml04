{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four: Multi-Layer Perceptron\n",
    "## Caleb Moore, Blake Gebhardt, Christian Gould\n",
    "dataset: https://www.kaggle.com/datasets/muonneutrino/us-census-demographic-data\n",
    "\n",
    "The classification task you will be performing is to predict, for each county, what the child poverty rate will be. You will need to convert this from regression to four levels of classification by quantizing the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import pinv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook setup\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Split, and Balance (1.5 points total)\n",
    "[.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part one\n",
    "df = pd.read_csv('./data/acs2015_county_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3218, 37)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part two \n",
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "# nice, not many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "State               object\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part three \n",
    "df.dtypes\n",
    "# lets take a better look at those state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states 52\n"
     ]
    }
   ],
   "source": [
    "print('number of states', len(df.State.unique()))\n",
    "df.State.unique()\n",
    "# States are redundant, since we have the county name, we can drop state\n",
    "df.drop('State', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of counties 1926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Autauga', 'Baldwin', 'Barbour', ..., 'Villalba', 'Yabucoa',\n",
       "       'Yauco'], dtype=object)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part four\n",
    "print('number of counties', len(df.County.unique()))\n",
    "df.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the counties\n",
    "\n",
    "# lets encode them\n",
    "initial_list = df.County.unique()\n",
    "codes = {initial_list[i]: i for i in range(len(initial_list))}\n",
    "\n",
    "df = df.replace(codes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 544 different ChildPoverty values\n",
      "we will want 804 ish instances in each class to create somewhat equal quartiles\n",
      "third_quartile     871\n",
      "first_quartile     807\n",
      "fourth_quartile    778\n",
      "second_quartile    762\n",
      "Name: poverty_quartile, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('there are',len(df[\"ChildPoverty\"].unique()), 'different ChildPoverty values')\n",
    "# lets try and simplify those into four classes\n",
    "print('we will want',int(len(df)/4), 'ish instances in each class to create somewhat equal quartiles')\n",
    "\n",
    "def categorise(row):  \n",
    "    if row['ChildPoverty'] > 0 and row['ChildPoverty'] <= 16:\n",
    "        return 'fourth_quartile'\n",
    "    elif row['ChildPoverty'] > 16 and row['ChildPoverty'] <= 23:\n",
    "        return 'third_quartile'\n",
    "    elif row['ChildPoverty'] > 23  and row['ChildPoverty'] <= 30:\n",
    "        return 'second_quartile'\n",
    "    return 'first_quartile'\n",
    "\n",
    "df['poverty_quartile'] = df.apply(lambda row: categorise(row), axis=1)\n",
    "\n",
    "print(df['poverty_quartile'].value_counts())\n",
    "# yes, we will want to balance on the set as a whole to ensure we do not skew our model compared to what it is going to attempt to predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is NO NEED to split the data multiple times for this lab.\n",
    "\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2574, 35)\n",
      "(2574, 4)\n",
      "(644, 35)\n",
      "(644, 4)\n"
     ]
    }
   ],
   "source": [
    "y = df['poverty_quartile']\n",
    "#one hot encode the poverty quartile\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['poverty_quartile', 'ChildPoverty']), y, test_size=0.20, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is take from Dr. Larsen's Notebook 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "# Start with the following functions:\n",
    "#    init\n",
    "#    encode_labels\n",
    "#    initialize weights\n",
    "#    sigmoid\n",
    "#    add bias (vector of ones)\n",
    "#    objective function (cost and regularizer)\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            \n",
    "        return self\n",
    "\n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert data into format to test accuracy with\n",
    "def convert_data(y_pred, y_test):\n",
    "  returnMe = pd.DataFrame()\n",
    "  \n",
    "  returnMe['first_quartile'] = (y_pred == 0)\n",
    "  returnMe['second_quartile'] = (y_pred == 1)\n",
    "  returnMe['third_quartile'] = (y_pred == 2)\n",
    "  returnMe['fourth_quartile'] = (y_pred == 3)\n",
    "\n",
    "  return returnMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3695652173913043\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = TwoLayerPerceptronVectorized(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred, y_test)\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      first_quartile  fourth_quartile  second_quartile  third_quartile\n",
      "2191               0                0                1               0\n",
      "791                0                1                0               0\n",
      "2385               0                1                0               0\n",
      "161                0                0                1               0\n",
      "255                0                0                0               1\n",
      "...              ...              ...              ...             ...\n",
      "2689               0                0                0               1\n",
      "2405               0                0                0               1\n",
      "1081               0                0                1               0\n",
      "2370               0                1                0               0\n",
      "2264               0                1                0               0\n",
      "\n",
      "[644 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7018633540372671\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define the TwoLayerPerceptron and TwoLayerPerceptronBase classes here\n",
    "\n",
    "class NormalizedTwoLayerPerceptron(TwoLayerPerceptronVectorized):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        scaler = StandardScaler()\n",
    "        X_norm = scaler.fit_transform(X)\n",
    "        super().fit(X_norm, y, print_progress=print_progress)\n",
    "        self.scaler_ = scaler\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_norm = self.scaler_.transform(X)\n",
    "        return super().predict(X_norm)\n",
    "\n",
    "# train the model\n",
    "model = NormalizedTwoLayerPerceptron(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred, y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>...</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>poverty_quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>third_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>third_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>2</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "      <td>first_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>3</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>second_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>4</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>second_quartile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId  County  TotalPop    Men  Women  Hispanic  White  Black  Native  \\\n",
       "0      1001       0     55221  26745  28476       2.6   75.8   18.5     0.4   \n",
       "1      1003       1    195121  95314  99807       4.5   83.1    9.5     0.6   \n",
       "2      1005       2     26932  14497  12435       4.6   46.2   46.7     0.2   \n",
       "3      1007       3     22604  12073  10531       2.2   74.5   21.4     0.4   \n",
       "4      1009       4     57710  28512  29198       8.6   87.9    1.5     0.3   \n",
       "\n",
       "   Asian  ...  OtherTransp  WorkAtHome  MeanCommute  Employed  PrivateWork  \\\n",
       "0    1.0  ...          1.3         1.8         26.5     23986         73.6   \n",
       "1    0.7  ...          1.4         3.9         26.4     85953         81.5   \n",
       "2    0.4  ...          1.5         1.6         24.1      8597         71.8   \n",
       "3    0.1  ...          1.5         0.7         28.8      8294         76.8   \n",
       "4    0.1  ...          0.4         2.3         34.9     22189         82.0   \n",
       "\n",
       "   PublicWork  SelfEmployed  FamilyWork  Unemployment  poverty_quartile  \n",
       "0        20.9           5.5         0.0           7.6    third_quartile  \n",
       "1        12.3           5.8         0.4           7.5    third_quartile  \n",
       "2        20.8           7.3         0.1          17.6    first_quartile  \n",
       "3        16.1           6.7         0.4           8.3   second_quartile  \n",
       "4        13.5           4.2         0.4           7.7   second_quartile  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['poverty_quartile']\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "X_categorical = df[['County']]\n",
    "X = df.drop(['poverty_quartile', 'ChildPoverty', 'County'], axis=1)\n",
    "\n",
    "# Normalize the numeric feature data\n",
    "X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "X_categorical = pd.get_dummies(X_categorical)\n",
    "\n",
    "# Concatenate the numeric and categorical features\n",
    "X = np.concatenate((X_norm, X_categorical), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7375776397515528\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = NormalizedTwoLayerPerceptron(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred, y_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing performance\n",
    "\n",
    "### First Model\n",
    "* The first model was able to get a decent score of about 38% accuracy. This is a good start, but we can do better. We were able to successfully beat a random choice, which would get about 25% accuracy, but we want to do better than that. The first model does not have any normalization or one-hot encoding on the input data, so there are some columns that are taking on a much larger weight than others. This is why we see such a large difference in accuracy between the first and second model.\n",
    "\n",
    "### Second Model\n",
    "* The second model was able to get a score of about 70% accuracy. This is a huge improvement over the first model. All we did was add a normalization! This is a great example of how important normalization is. The second model is still not as good as the third model, since the second model does not one-hot encode the colunn of county. That is only one column, so the difference in accuracy is not as large as the difference between the first and second model.\n",
    "\n",
    "### Third Model\n",
    "* The third model was able to get a score of about 73% accuracy. The slight improvement is because of the one hot encoding of the county column. This is a great example of how important one-hot encoding is. If there were many more columns that were not one-hot encoded, the difference in accuracy would be much larger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (5 points total)\n",
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptronBase(TwoLayerPerceptronBase):\n",
    "    def __init__(self, n_hidden_1=30, n_hidden_2=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        super().__init__(n_hidden=n_hidden_1, C=C, epochs=epochs, eta=eta, random_state=random_state)\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1) * self.n_hidden_1\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden_1, self.n_features_ + 1)  # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n_hidden_1 + 1) * self.n_hidden_2\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_hidden_2, self.n_hidden_1 + 1)\n",
    "\n",
    "        W3_num_elems = (self.n_hidden_2 + 1) * self.n_output_\n",
    "        W3 = np.random.uniform(-1.0, 1.0, size=W3_num_elems)\n",
    "        W3 = W3.reshape(self.n_output_, self.n_hidden_2 + 1)\n",
    "\n",
    "        return W1, W2, W3\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self, A4, Y_enc, W1, W2, W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc - A4) ** 2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptron(ThreeLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, W3):\n",
    "        \"\"\"Compute feedforward step\"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        V3 = -2 * (Y_enc - A4) * A4 * (1 - A4)\n",
    "        V2 = A3 * (1 - A3) * (W3.T @ V3)\n",
    "        V1 = A2 * (1 - A2) * (W2.T @ V2[1:, :])\n",
    "\n",
    "        grad3 = V3 @ A3.T\n",
    "        grad2 = V2[1:, :] @ A2.T\n",
    "        grad1 = V1[1:, :] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2, grad3\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': [], 'W3': []}  # Added line to store gradient magnitudes\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data, self.W1, self.W2, self.W3)\n",
    "\n",
    "            cost = self._cost(A4, Y_enc, self.W1, self.W2, self.W3)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grad1, grad2, grad3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, Z1=Z1, Z2=Z2, Z3=Z3, Y_enc=Y_enc,\n",
    "                                                     W1=self.W1, W2=self.W2, W3=self.W3)\n",
    "\n",
    "            # Save average gradient magnitudes\n",
    "            self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "            self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "            self.grad_mag_['W3'].append(np.mean(np.abs(grad3)))\n",
    "\n",
    "            self.W1 -= self.eta * grad1\n",
    "            self.W2 -= self.eta * grad2\n",
    "            self.W3 -= self.eta * grad3\n",
    "\n",
    "        return self\n",
    "\n",
    "    def plot_gradient_magnitudes(self):\n",
    "        \"\"\"Plot the average gradient magnitudes for each layer.\"\"\"\n",
    "        plt.plot(self.grad_mag_['W1'], label='W1', linestyle='-', marker='o')\n",
    "        plt.plot(self.grad_mag_['W2'], label='W2', linestyle='-', marker='s')\n",
    "        plt.plot(self.grad_mag_['W3'], label='W3', linestyle='-', marker='d')\n",
    "        \n",
    "        plt.ylabel('Average Gradient Magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the three-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1 points] Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (such as AdaGrad, RMSProps, or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (1 points total)\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4045ce629ca2404df8e09a4bb72649c60795682d1cc7b69b21add64452ef6fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
