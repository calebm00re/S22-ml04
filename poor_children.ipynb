{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four: Multi-Layer Perceptron\n",
    "## Caleb Moore, Blake Gebhardt, Christian Gould\n",
    "dataset: https://www.kaggle.com/datasets/muonneutrino/us-census-demographic-data\n",
    "\n",
    "The classification task you will be performing is to predict, for each county, what the child poverty rate will be. You will need to convert this from regression to four levels of classification by quantizing the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import pinv\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook setup\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Split, and Balance (1.5 points total)\n",
    "[.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part one\n",
    "df = pd.read_csv('./data/acs2015_county_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3218, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part two \n",
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "# nice, not many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "State               object\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part three \n",
    "df.dtypes\n",
    "# lets take a better look at those state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n",
       "       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming', 'Puerto Rico'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of states', len(df.State.unique()))\n",
    "df.State.unique()\n",
    "# assuming states play a role in poverty rates, lets put these into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>0</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId  State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001      0  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003      0  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005      0  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007      0     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009      0   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets encode them\n",
    "codes = { 'Alabama': 0, 'Alaska': 1, 'Arizona': 2, 'Arkansas': 3, 'California': 4,\n",
    "       'Colorado': 5, 'Connecticut': 6, 'Delaware': 7, 'District of Columbia': 8,\n",
    "       'Florida': 9, 'Georgia': 10, 'Hawaii': 11, 'Idaho': 12, 'Illinois': 13, 'Indiana': 14,\n",
    "       'Iowa': 15, 'Kansas': 16, 'Kentucky': 17, 'Louisiana': 18, 'Maine': 19, 'Maryland': 20,\n",
    "       'Massachusetts': 21, 'Michigan': 22, 'Minnesota': 23, 'Mississippi': 24,\n",
    "       'Missouri': 25, 'Montana': 26, 'Nebraska': 27, 'Nevada': 28, 'New Hampshire': 29,\n",
    "       'New Jersey': 30, 'New Mexico': 31, 'New York': 32, 'North Carolina': 33,\n",
    "       'North Dakota': 34, 'Ohio': 35, 'Oklahoma': 36, 'Oregon': 37, 'Pennsylvania': 38,\n",
    "       'Rhode Island': 39, 'South Carolina': 40, 'South Dakota': 41, 'Tennessee': 42,\n",
    "       'Texas': 43, 'Utah': 44, 'Vermont': 45, 'Virginia': 46, 'Washington': 47,\n",
    "       'West Virginia': 48, 'Wisconsin': 49, 'Wyoming': 50, 'Puerto Rico': 51 }\n",
    "\n",
    "df = df.replace(codes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "State                int64\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states 1926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Autauga', 'Baldwin', 'Barbour', ..., 'Villalba', 'Yabucoa',\n",
       "       'Yauco'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part four\n",
    "print('number of counties', len(df.County.unique()))\n",
    "df.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>0</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId  State  TotalPop    Men  Women  Hispanic  White  Black  Native  \\\n",
       "0      1001      0     55221  26745  28476       2.6   75.8   18.5     0.4   \n",
       "1      1003      0    195121  95314  99807       4.5   83.1    9.5     0.6   \n",
       "2      1005      0     26932  14497  12435       4.6   46.2   46.7     0.2   \n",
       "3      1007      0     22604  12073  10531       2.2   74.5   21.4     0.4   \n",
       "4      1009      0     57710  28512  29198       8.6   87.9    1.5     0.3   \n",
       "\n",
       "   Asian  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0    1.0  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1    0.7  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2    0.4  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3    0.1  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4    0.1  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We think we will get rid of the county variable since there are almost two thousand unique values it might not be very useful to encode it\n",
    "df.drop(columns=['County'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is NO NEED to split the data multiple times for this lab.\n",
    "\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (5 points total)\n",
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1 points] Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (such as AdaGrad, RMSProps, or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (1 points total)\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4045ce629ca2404df8e09a4bb72649c60795682d1cc7b69b21add64452ef6fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
