{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four: Multi-Layer Perceptron\n",
    "## Caleb Moore, Blake Gebhardt, Christian Gould\n",
    "dataset: https://www.kaggle.com/datasets/muonneutrino/us-census-demographic-data\n",
    "\n",
    "The classification task you will be performing is to predict, for each county, what the child poverty rate will be. You will need to convert this from regression to four levels of classification by quantizing the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import pinv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebook setup\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Split, and Balance (1.5 points total)\n",
    "[.5 points] (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. (2) Remove any observations that having missing data. (3) Encode any string data as integers for now. (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part one\n",
    "df = pd.read_csv('./data/acs2015_county_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3218, 37)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part two \n",
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "# nice, not many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "State               object\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part three \n",
    "df.dtypes\n",
    "# lets take a better look at those state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of states 52\n"
     ]
    }
   ],
   "source": [
    "print('number of states', len(df.State.unique()))\n",
    "df.State.unique()\n",
    "# States are redundant, since we have the county name, we can drop state\n",
    "df.drop('State', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CensusId             int64\n",
       "County              object\n",
       "TotalPop             int64\n",
       "Men                  int64\n",
       "Women                int64\n",
       "Hispanic           float64\n",
       "White              float64\n",
       "Black              float64\n",
       "Native             float64\n",
       "Asian              float64\n",
       "Pacific            float64\n",
       "Citizen              int64\n",
       "Income             float64\n",
       "IncomeErr          float64\n",
       "IncomePerCap         int64\n",
       "IncomePerCapErr      int64\n",
       "Poverty            float64\n",
       "ChildPoverty       float64\n",
       "Professional       float64\n",
       "Service            float64\n",
       "Office             float64\n",
       "Construction       float64\n",
       "Production         float64\n",
       "Drive              float64\n",
       "Carpool            float64\n",
       "Transit            float64\n",
       "Walk               float64\n",
       "OtherTransp        float64\n",
       "WorkAtHome         float64\n",
       "MeanCommute        float64\n",
       "Employed             int64\n",
       "PrivateWork        float64\n",
       "PublicWork         float64\n",
       "SelfEmployed       float64\n",
       "FamilyWork         float64\n",
       "Unemployment       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of counties 1926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Autauga', 'Baldwin', 'Barbour', ..., 'Villalba', 'Yabucoa',\n",
       "       'Yauco'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part four\n",
    "print('number of counties', len(df.County.unique()))\n",
    "df.County.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the counties\n",
    "\n",
    "# lets encode them\n",
    "initial_list = df.County.unique()\n",
    "codes = {initial_list[i]: i for i in range(len(initial_list))}\n",
    "\n",
    "df = df.replace(codes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. Should balancing of the dataset be done for both the training and testing set? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 544 different ChildPoverty values\n",
      "we will want 804 ish instances in each class to create somewhat equal quartiles\n",
      "third_quartile     871\n",
      "first_quartile     807\n",
      "fourth_quartile    778\n",
      "second_quartile    762\n",
      "Name: poverty_quartile, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('there are',len(df[\"ChildPoverty\"].unique()), 'different ChildPoverty values')\n",
    "# lets try and simplify those into four classes\n",
    "print('we will want',int(len(df)/4), 'ish instances in each class to create somewhat equal quartiles')\n",
    "\n",
    "def categorise(row):  \n",
    "    if row['ChildPoverty'] > 0 and row['ChildPoverty'] <= 16:\n",
    "        return 'fourth_quartile'\n",
    "    elif row['ChildPoverty'] > 16 and row['ChildPoverty'] <= 23:\n",
    "        return 'third_quartile'\n",
    "    elif row['ChildPoverty'] > 23  and row['ChildPoverty'] <= 30:\n",
    "        return 'second_quartile'\n",
    "    return 'first_quartile'\n",
    "\n",
    "df['poverty_quartile'] = df.apply(lambda row: categorise(row), axis=1)\n",
    "\n",
    "print(df['poverty_quartile'].value_counts())\n",
    "# yes, we will want to balance on the set as a whole to ensure we do not skew our model compared to what it is going to attempt to predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is NO NEED to split the data multiple times for this lab.\n",
    "\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2574, 35)\n",
      "(2574, 4)\n",
      "(644, 35)\n",
      "(644, 4)\n"
     ]
    }
   ],
   "source": [
    "y = df['poverty_quartile']\n",
    "#one hot encode the poverty quartile\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['poverty_quartile', 'ChildPoverty']), y, test_size=0.20, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is take from Dr. Larsen's Notebook 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "# Start with the following functions:\n",
    "#    init\n",
    "#    encode_labels\n",
    "#    initialize weights\n",
    "#    sigmoid\n",
    "#    add bias (vector of ones)\n",
    "#    objective function (cost and regularizer)\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using the Glorot initialization for a two-layer perceptron.\"\"\"\n",
    "        glorot = lambda n_in, n_out: np.sqrt(6 / (n_in + n_out))\n",
    "\n",
    "        W1_shape = (self.n_hidden, self.n_features_ + 1)\n",
    "        W1 = np.random.uniform(-glorot(self.n_features_, self.n_hidden), glorot(self.n_features_, self.n_hidden), size=W1_shape)\n",
    "\n",
    "        W2_shape = (self.n_output_, self.n_hidden + 1)\n",
    "        W2 = np.random.uniform(-glorot(self.n_hidden, self.n_output_), glorot(self.n_hidden, self.n_output_), size=W2_shape)\n",
    "\n",
    "        return W1, W2\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, batch_size=32):\n",
    "        \"\"\"Learn weights from training data using mini-batch gradient descent.\"\"\"\n",
    "\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': []}  # Added line to store gradient magnitudes\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # shuffle the data before each epoch\n",
    "            idx = np.random.permutation(X_data.shape[0])\n",
    "            X_data = X_data[idx]\n",
    "            Y_enc = Y_enc[:, idx]\n",
    "\n",
    "            mini_batch_indices = np.array_split(np.arange(X_data.shape[0]), X_data.shape[0] // batch_size)\n",
    "\n",
    "            for batch_indices in mini_batch_indices:\n",
    "                X_mini_batch = X_data[batch_indices]\n",
    "                Y_enc_mini_batch = Y_enc[:, batch_indices]\n",
    "\n",
    "                # feedforward all instances in the mini-batch\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_mini_batch, self.W1, self.W2)\n",
    "\n",
    "                cost = self._cost(A3, Y_enc_mini_batch, self.W1, self.W2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc_mini_batch,\n",
    "                                                  W1=self.W1, W2=self.W2)\n",
    "\n",
    "                # Save average gradient magnitudes\n",
    "                self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "                self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "\n",
    "                self.W1 -= self.eta * grad1\n",
    "                self.W2 -= self.eta * grad2\n",
    "\n",
    "        return self\n",
    "\n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert data into format to test accuracy with\n",
    "def convert_data(y_pred):\n",
    "  returnMe = pd.DataFrame()\n",
    "  \n",
    "  returnMe['first_quartile'] = (y_pred == 0)\n",
    "  returnMe['second_quartile'] = (y_pred == 1)\n",
    "  returnMe['third_quartile'] = (y_pred == 2)\n",
    "  returnMe['fourth_quartile'] = (y_pred == 3)\n",
    "\n",
    "  return returnMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([ 817,  391, 2409, 1329,  401,  683, 1848,  512, 1505, 1592,\\n            ...\\n              38, 2444, 2000,  515,  737, 1453, 2006,  569, 2506, 2126],\\n           dtype='int64', length=2574)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/0617f_bj495b6mtgzb9681gr0000gn/T/ipykernel_54276/338891679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerPerceptronVectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot the loss over time to ensure convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ck/0617f_bj495b6mtgzb9681gr0000gn/T/ipykernel_54276/3791870289.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress, batch_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# shuffle the data before each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mY_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5794\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5798\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5854\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5855\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5856\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5858\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([ 817,  391, 2409, 1329,  401,  683, 1848,  512, 1505, 1592,\\n            ...\\n              38, 2444, 2000,  515,  737, 1453, 2006,  569, 2506, 2126],\\n           dtype='int64', length=2574)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = TwoLayerPerceptronVectorized(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred)\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      first_quartile  fourth_quartile  second_quartile  third_quartile\n",
      "2191               0                0                1               0\n",
      "791                0                1                0               0\n",
      "2385               0                1                0               0\n",
      "161                0                0                1               0\n",
      "255                0                0                0               1\n",
      "...              ...              ...              ...             ...\n",
      "2689               0                0                0               1\n",
      "2405               0                0                0               1\n",
      "1081               0                0                1               0\n",
      "2370               0                1                0               0\n",
      "2264               0                1                0               0\n",
      "\n",
      "[644 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7018633540372671\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NormalizedTwoLayerPerceptron(TwoLayerPerceptronVectorized):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        scaler = StandardScaler()\n",
    "        X_norm = scaler.fit_transform(X)\n",
    "        super().fit(X_norm, y, print_progress=print_progress)\n",
    "        self.scaler_ = scaler\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_norm = self.scaler_.transform(X)\n",
    "        return super().predict(X_norm)\n",
    "\n",
    "# train the model\n",
    "model = NormalizedTwoLayerPerceptron(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>...</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>poverty_quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>third_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>third_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>2</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "      <td>first_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>3</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>second_quartile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>4</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>second_quartile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId  County  TotalPop    Men  Women  Hispanic  White  Black  Native  \\\n",
       "0      1001       0     55221  26745  28476       2.6   75.8   18.5     0.4   \n",
       "1      1003       1    195121  95314  99807       4.5   83.1    9.5     0.6   \n",
       "2      1005       2     26932  14497  12435       4.6   46.2   46.7     0.2   \n",
       "3      1007       3     22604  12073  10531       2.2   74.5   21.4     0.4   \n",
       "4      1009       4     57710  28512  29198       8.6   87.9    1.5     0.3   \n",
       "\n",
       "   Asian  ...  OtherTransp  WorkAtHome  MeanCommute  Employed  PrivateWork  \\\n",
       "0    1.0  ...          1.3         1.8         26.5     23986         73.6   \n",
       "1    0.7  ...          1.4         3.9         26.4     85953         81.5   \n",
       "2    0.4  ...          1.5         1.6         24.1      8597         71.8   \n",
       "3    0.1  ...          1.5         0.7         28.8      8294         76.8   \n",
       "4    0.1  ...          0.4         2.3         34.9     22189         82.0   \n",
       "\n",
       "   PublicWork  SelfEmployed  FamilyWork  Unemployment  poverty_quartile  \n",
       "0        20.9           5.5         0.0           7.6    third_quartile  \n",
       "1        12.3           5.8         0.4           7.5    third_quartile  \n",
       "2        20.8           7.3         0.1          17.6    first_quartile  \n",
       "3        16.1           6.7         0.4           8.3   second_quartile  \n",
       "4        13.5           4.2         0.4           7.7   second_quartile  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['poverty_quartile']\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "X_categorical = df[['County']]\n",
    "X = df.drop(['poverty_quartile', 'ChildPoverty', 'County'], axis=1)\n",
    "\n",
    "# Normalize the numeric feature data\n",
    "X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "# One-hot encode the categorical features\n",
    "X_categorical = pd.get_dummies(X_categorical)\n",
    "\n",
    "# Concatenate the numeric and categorical features\n",
    "X = np.concatenate((X_norm, X_categorical), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7375776397515528\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = NormalizedTwoLayerPerceptron(n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=42)\n",
    "model.fit(X_train, y_train, print_progress=True)\n",
    "\n",
    "# plot the loss over time to ensure convergence\n",
    "plt.plot(range(1, len(model.cost_)+1), model.cost_)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert y_pred into a one-hot encoded dataframe\n",
    "y_pred = convert_data(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing performance\n",
    "\n",
    "### First Model\n",
    "* The first model was able to get a decent score of about 38% accuracy. This is a good start, but we can do better. We were able to successfully beat a random choice, which would get about 25% accuracy, but we want to do better than that. The first model does not have any normalization or one-hot encoding on the input data, so there are some columns that are taking on a much larger weight than others. This is why we see such a large difference in accuracy between the first and second model.\n",
    "\n",
    "### Second Model\n",
    "* The second model was able to get a score of about 70% accuracy. This is a huge improvement over the first model. All we did was add a normalization! This is a great example of how important normalization is. The second model is still not as good as the third model, since the second model does not one-hot encode the colunn of county. That is only one column, so the difference in accuracy is not as large as the difference between the first and second model.\n",
    "\n",
    "### Third Model\n",
    "* The third model was able to get a score of about 73% accuracy. The slight improvement is because of the one hot encoding of the county column. This is a great example of how important one-hot encoding is. If there were many more columns that were not one-hot encoded, the difference in accuracy would be much larger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (5 points total)\n",
    "[1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptronBase(TwoLayerPerceptronBase):\n",
    "    def __init__(self, n_hidden_1=30, n_hidden_2=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        super().__init__(n_hidden=n_hidden_1, C=C, epochs=epochs, eta=eta, random_state=random_state)\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using the Glorot initialization for a three-layer perceptron.\"\"\"\n",
    "        glorot = lambda n_in, n_out: np.sqrt(6 / (n_in + n_out))\n",
    "\n",
    "        W1_shape = (self.n_hidden_1, self.n_features_ + 1)\n",
    "        W1 = np.random.uniform(-glorot(self.n_features_, self.n_hidden_1), glorot(self.n_features_, self.n_hidden_1), size=W1_shape)\n",
    "\n",
    "        W2_shape = (self.n_hidden_2, self.n_hidden_1 + 1)\n",
    "        W2 = np.random.uniform(-glorot(self.n_hidden_1, self.n_hidden_2), glorot(self.n_hidden_1, self.n_hidden_2), size=W2_shape)\n",
    "\n",
    "        W3_shape = (self.n_output_, self.n_hidden_2 + 1)\n",
    "        W3 = np.random.uniform(-glorot(self.n_hidden_2, self.n_output_), glorot(self.n_hidden_2, self.n_output_), size=W3_shape)\n",
    "\n",
    "        return W1, W2, W3\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2) + np.mean(W3[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self, A4, Y_enc, W1, W2, W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc - A4) ** 2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptron(ThreeLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, W3):\n",
    "        \"\"\"Compute feedforward step\"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        V3 = -2 * (Y_enc - A4) * A4 * (1 - A4)\n",
    "        V2 = A3 * (1 - A3) * (W3.T @ V3)\n",
    "        V1 = A2 * (1 - A2) * (W2.T @ V2[1:, :])\n",
    "\n",
    "        grad3 = V3 @ A3.T\n",
    "        grad2 = V2[1:, :] @ A2.T\n",
    "        grad1 = V1[1:, :] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2, grad3\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, batch_size=32):\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': [], 'W3': []}  # Added line to store gradient magnitudes\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # shuffle the data before each epoch\n",
    "            idx = np.random.permutation(X_data.shape[0])\n",
    "            X_data = X_data[idx]\n",
    "            Y_enc = Y_enc[:, idx]\n",
    "\n",
    "            mini_batch_indices = np.array_split(np.arange(X_data.shape[0]), X_data.shape[0] // batch_size)\n",
    "\n",
    "            for batch_indices in mini_batch_indices:\n",
    "                X_mini_batch = X_data[batch_indices]\n",
    "                Y_enc_mini_batch = Y_enc[:, batch_indices]\n",
    "\n",
    "                # feedforward all instances in the mini-batch\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_mini_batch, self.W1, self.W2, self.W3)\n",
    "\n",
    "                cost = self._cost(A4, Y_enc_mini_batch, self.W1, self.W2, self.W3)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2, grad3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4,\n",
    "                                                        Z1=Z1, Z2=Z2, Z3=Z3,\n",
    "                                                        Y_enc=Y_enc_mini_batch,\n",
    "                                                        W1=self.W1, W2=self.W2, W3=self.W3)\n",
    "\n",
    "                # Save average gradient magnitudes\n",
    "                self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "                self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "                self.grad_mag_['W3'].append(np.mean(np.abs(grad3)))\n",
    "\n",
    "                self.W1 -= self.eta * grad1\n",
    "                self.W2 -= self.eta * grad2\n",
    "                self.W3 -= self.eta * grad3\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def plot_gradient_magnitudes(self):\n",
    "        \"\"\"Plot the average gradient magnitudes for each layer.\"\"\"\n",
    "        plt.plot(self.grad_mag_['W1'], label='W1', linestyle='-', marker='o')\n",
    "        plt.plot(self.grad_mag_['W2'], label='W2', linestyle='-', marker='s')\n",
    "        plt.plot(self.grad_mag_['W3'], label='W3', linestyle='-', marker='d')\n",
    "        \n",
    "        plt.ylabel('Average Gradient Magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the three-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 67/500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/0617f_bj495b6mtgzb9681gr0000gn/T/ipykernel_54276/1884565377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run the three layer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreeLayerPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_gradient_magnitudes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ck/0617f_bj495b6mtgzb9681gr0000gn/T/ipykernel_54276/3584057221.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress, batch_size)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;31m# compute gradient via backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 grad1, grad2, grad3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4,\n\u001b[0m\u001b[1;32m     76\u001b[0m                                                         \u001b[0mZ1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                                                         \u001b[0mY_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_enc_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ck/0617f_bj495b6mtgzb9681gr0000gn/T/ipykernel_54276/3584057221.py\u001b[0m in \u001b[0;36m_get_gradient\u001b[0;34m(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgrad3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV3\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgrad2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mgrad1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# regularize weights that are not bias terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run the three layer model\n",
    "nn = ThreeLayerPerceptron(n_hidden_1=50, n_hidden_2=50, C=0.0, epochs=500, eta=0.001, random_state=1)\n",
    "nn.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "nn.plot_gradient_magnitudes()\n",
    "\n",
    "# Check the accuracy\n",
    "y_pred = nn.predict(X_test)\n",
    "y_pred = convert_data(y_pred)\n",
    "print('Accuracy: %.2f%%' % (100 * accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerPerceptron(ThreeLayerPerceptron):\n",
    "    def __init__(self, n_hidden1=30, n_hidden2=30, n_hidden3=30, epochs=500, eta=0.001, random_state=None, l2_C=0.0):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.l2_C = l2_C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, W4):\n",
    "        \"\"\"Compute feedforward step\"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        Z3 = W3 @ A3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        A4 = self._add_bias_unit(A4, how='row')\n",
    "        Z4 = W4 @ A4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, Z1, Z2, Z3, Z4, Y_enc, W1, W2, W3, W4):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        V4 = -2 * (Y_enc - A5) * A5 * (1 - A5)\n",
    "        V3 = A4 * (1 - A4) * (W4.T @ V4)\n",
    "        V2 = A3 * (1 - A3) * (W3.T @ V3[1:, :])\n",
    "        V1 = A2 * (1 - A2) * (W2.T @ V2[1:, :])\n",
    "\n",
    "        grad4 = V4 @ A4.T\n",
    "        grad3 = V3[1:, :] @ A3.T\n",
    "        grad2 = V2[1:, :] @ A2.T\n",
    "        grad1 = V1[1:, :] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2, grad3, grad4\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using the Glorot initialization for a four-layer perceptron.\"\"\"\n",
    "        glorot = lambda n_in, n_out: np.sqrt(6 / (n_in + n_out))\n",
    "\n",
    "        W1_shape = (self.n_hidden1, self.n_features_ + 1)\n",
    "        W1 = np.random.uniform(-glorot(self.n_features_, self.n_hidden1), glorot(self.n_features_, self.n_hidden1), size=W1_shape)\n",
    "\n",
    "        W2_shape = (self.n_hidden2, self.n_hidden1 + 1)\n",
    "        W2 = np.random.uniform(-glorot(self.n_hidden1, self.n_hidden2), glorot(self.n_hidden1, self.n_hidden2), size=W2_shape)\n",
    "\n",
    "        W3_shape = (self.n_hidden3, self.n_hidden2 + 1)\n",
    "        W3 = np.random.uniform(-glorot(self.n_hidden2, self.n_hidden3), glorot(self.n_hidden2, self.n_hidden3), size=W3_shape)\n",
    "\n",
    "        W4_shape = (self.n_output_, self.n_hidden3 + 1)\n",
    "        W4 = np.random.uniform(-glorot(self.n_hidden3, self.n_output_), glorot(self.n_hidden3, self.n_output_), size=W4_shape)\n",
    "\n",
    "        return W1, W2, W3, W4\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, A5 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4)\n",
    "        y_pred = np.argmax(A5, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, batch_size=32):\n",
    "        \"\"\"Learn weights from training data using mini-batch gradient descent.\"\"\"\n",
    "\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': [], 'W3': [], 'W4': []}  # Added line to store gradient magnitudes\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # shuffle the data before each epoch\n",
    "            idx = np.random.permutation(X_data.shape[0])\n",
    "            X_data = X_data[idx]\n",
    "            Y_enc = Y_enc[:, idx]\n",
    "\n",
    "            mini_batch_indices = np.array_split(np.arange(X_data.shape[0]), X_data.shape[0] // batch_size)\n",
    "\n",
    "            for batch_indices in mini_batch_indices:\n",
    "                X_mini_batch = X_data[batch_indices]\n",
    "                Y_enc_mini_batch = Y_enc[:, batch_indices]\n",
    "\n",
    "                # feedforward all instances in the mini-batch\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5 = self._feedforward(X_mini_batch, self.W1, self.W2, self.W3, self.W4)\n",
    "\n",
    "                cost = self._cost(A5, Y_enc_mini_batch, self.W1, self.W2, self.W3, self.W4)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2, grad3, grad4 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5,\n",
    "                                                                Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4,\n",
    "                                                                Y_enc=Y_enc_mini_batch,\n",
    "                                                                W1=self.W1, W2=self.W2, W3=self.W3, W4=self.W4)\n",
    "\n",
    "                # Save average gradient magnitudes\n",
    "                self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "                self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "                self.grad_mag_['W3'].append(np.mean(np.abs(grad3)))\n",
    "                self.grad_mag_['W4'].append(np.mean(np.abs(grad4)))\n",
    "\n",
    "                self.W1 -= self.eta * grad1\n",
    "                self.W2 -= self.eta * grad2\n",
    "                self.W3 -= self.eta * grad3\n",
    "                self.W4 -= self.eta * grad4\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _cost(self, A5, Y_enc, W1, W2, W3, W4):\n",
    "        \"\"\"Compute the cost function.\"\"\"\n",
    "        cost = np.mean((Y_enc - A5)**2)\n",
    "        L2_term = (self.l2_C * (np.sum(W1[:, 1:] ** 2) +\n",
    "                                np.sum(W2[:, 1:] ** 2) +\n",
    "                                np.sum(W3[:, 1:] ** 2) +\n",
    "                                np.sum(W4[:, 1:] ** 2)))\n",
    "        return cost + L2_term\n",
    "\n",
    "    def plot_gradient_magnitudes(self):\n",
    "        \"\"\"Plot the average gradient magnitudes for each layer.\"\"\"\n",
    "        plt.plot(self.grad_mag_['W1'], label='W1', linestyle='-', marker='o')\n",
    "        plt.plot(self.grad_mag_['W2'], label='W2', linestyle='-', marker='s')\n",
    "        plt.plot(self.grad_mag_['W3'], label='W3', linestyle='-', marker='d')\n",
    "        plt.plot(self.grad_mag_['W4'], label='W4', linestyle='-', marker='x')\n",
    "\n",
    "        plt.ylabel('Average Gradient Magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABXMUlEQVR4nO2deXxU1dn4v89MJmQhAUzYwyYoKiAgVBZbBa2iWLW2rn3biooUFFFaa+3bt2h9+3tftSrVugIqbd9WXFpccauCS1kUFGRXUAwJS0ggJJlkssw8vz/unclMMpncJDNZyPl+Pvcz95577jnPPTNznnOec85zRFUxGAwGQ+fF1dYCGAwGg6FtMYrAYDAYOjlGERgMBkMnxygCg8Fg6OQYRWAwGAydnKS2FqCpZGdn6+DBg5v1rNfrJT09Pb4CxYH2Khe0X9mMXE3DyNU0jkW5NmzYUKiqPaPeVNUOdYwbN06by8qVK5v9bCJpr3Kptl/ZjFxNw8jVNI5FuYD12kC9akxDBoPB0MkxisBgMBg6OUYRGAwGQyenww0WGwwGQ7yprq4mLy8Pn88HQLdu3di+fXsbS1UfJ3KlpKSQk5ODx+NxnK5RBAaDodOTl5dHRkYGgwcPRkQoLS0lIyOjrcWqR2NyqSpFRUXk5eUxZMgQx+ke86ahoiVL8K5dFxHmXbuOoiVL2kgig8HQ3vD5fGRlZSEibS1KixARsrKyQj0bpxzziiBl5Cjy588PKQPv2nXkz59PyshRbSyZwWBoT3R0JRCkOe+RMNOQiAwA/gL0AQLAIlV9qE6cKcDLwNd20D9V9e54ypE+cQL9Fy4kf/58ug0YQP7evfRfuJD0iRPimY3BYDB0WBLZI6gBfqGqJwMTgZtE5JQo8T5U1TH2EVclECR94gQ8AwaQ8vnnpE2aZJSAwWBoV8yfP58//vGPoetp06Yxc+bM0PUvfvELHnzwQS699FK6d+/O9773vbjmnzBFoKr7VfVT+7wU2A70T1R+sfCuXUfljh3W+fvv1xszMBgMhqbw0mf5nHHPewy543XOuOc9Xvosv0XpTZ48mdWrVwMQCAQoLCxk69atofurV6/mjDPO4JZbbuGvf/1ri/KKRqvMGhKRwcBYIFoNPElENgH7gNtUdWvdCCIyC5gF0Lt3b1atWuU4b8/OnXRfvISawYNJ/uILSs48kz1z51J8w0yqhw9vxtvEn7Kysia9U2vSXmUzcjUNI1dsunXrRmlpaeja7/dHXIfz+paD3PX6l/hqAgDkF1dwxz8+x+er4MKRvZuV/+jRo7n11lspLS1l69atDB8+nAMHDpCbm0taWhrbt29n2LBhDB8+nNWrV1NTU9OgfGANfjelXBOuCESkK/AP4FZVLalz+1NgkKqWich04CXghLppqOoiYBHA+PHjdcqUKY7zL9q1i5RHHqH4+ecp+eILTjznHJKuvBLfls1kNSGdRLJq1Sqa8k6tSXuVzcjVNIxcsdm+fXtoWubvXt3K5r1HcLvdUeN+lltMlT8QEearCbDgtS9Z/vmhqM+c0i+TOy8a0WD+GRkZeDwejhw5wqZNmzjzzDPJz89ny5YtdOvWjVNPPZWsrCxKS0tJS0sjKSkp5jTSlJQUxo4d29hrh0jorCER8WApgb+p6j/r3lfVElUts89XAB4RyY6nDFkzZ0aOCaiSPnECWWH2N4PBYHBKXSXQWLhTzjjjDFavXs3q1auZNGkSkyZNCl1Pnjy5RWk3RiJnDQnwFLBdVR9sIE4f4KCqqoicjqWYihIkUEKSNRgMxxZ3XjQi5sKtM+55j/ziinrh/bun8tzPJjU73+A4webNmxk5ciQDBgzggQceIDMzk+uuu67Z6TohkT2CM4CfAGeLyEb7mC4is0Vkth3nMmCLPUbwMHCV7S7VYDAY2iW/nDacVE+k2SjV4+aX01o25njGGWfw2muvcdxxx+F2uznuuOMoLi5mzZo1TJrUfAXjhIT1CFT1IyBmM1xVHwEeSZQMBoPBEG++P9aa/PiHt3ayr7iCft1T+eW04aHw5jJq1CgKCwv50Y9+FBFWVlZGdrZlMZ82bRpffvklZWVl5OTk8NRTTzFt2rQW5Qud0teQ6XAYDIaW8f2x/Vtc8dfF7XZTUhI5n2bp0qUR12+99VZCfCAd8y4mQpgxAoPBYIhK51EEBoPBYIiKUQQGg8HQyel8isBMSjIYDIYIOo8iMGMEBoPBEJXOowgMBoPBEBWjCAwGg6GNceqG+pxzzmHEiBGceuqpPPfcc3HLv/OtIzBjBAaDoSX84QTwFtQPT+8Fv/yyWUlOnjyZF154gVtvvTXkhjp8TcHq1av54x//yNSpUxk7diz79u1j3LhxTJs2je7duzfzRWrpPD0CM0RgMBjiQTQlECvcAUGHcwBbt25l5MiRZGRkcOTIESorK9m+fTtjx45l2LBhAPTr149evXpx6FB0b6dNpfP1CAwGgyEWb9xBav5n4G5G9fjMhdHD+4yCC+5p8LF+/fqRlJREbm5uyPtofn4+a9asCbmhTk5OprKyEoCPP/6Yqqoqhg4d2nQZo2AUgcFgMLQDwt1Q//znPyc/P5/Vq1fTrVu3CDfU+/fv5yc/+Ql//vOfcbniY9QxisBgMBjCueAeKmK4oeaubg0/e+3rzc7WiRvqkpISLrroIn7/+98zceLEZudVl84zRmBjvFwbDIb2SGNuqKuqqviP//gPfvrTn3L55ZfHNe9OowjELCgzGAzxIL1X08IdEnRDHd7SHzVqFN26dSM7O5vnn3+ef//73yxdupQxY8YwZswYNm7c2KI8gxjTkMFgMDSFZk4RbYzG3FD/+Mc/5pJLLjFuqA0Gg8EQfzqfIjBDBAaDwRBB51MEBoPBYIig8ykCM2ZsMBgMEXQ+RWAwGAyGCBwpAhH5tohca5/3FJEhiRUrgZgxAoPBYIigUUUgIncCvwJ+bQd5gP9LpFCJwdiEDAZD/Nh1ZBfff/n77Dqyq8VpOXFD/bvf/Y4zzzyTMWPGMGLECJ544okW5xvESY/gUuBiwAugqvuA+E9kNRgMhg5CeXU5N757I18Vf8VN795EeXV5i9ILupcAQm6ot27dGrq/evVqzj//fN555x02btzIunXruOeee9i3b1+L8g3iRBFUqeWXQQFEJD0uORsMBkMHZcHqBRz2HUZRiiqKuHP1nS1Kz6kb6i5dugBQWVlJIBBo8XsEcbKy+HkReRLoLiI3ANcBi+MmQWtjfA0ZDIYY3PvxvWw9tBW32x31/qHyQ+wt20tArYq4MlDJ23veZnvRdnqm9Yz6zEnHncSvTv9Vg3k6dUO9e/durrzySnbt2sUf/vAH+vXr1/IXxkGPQFXvB14E/gEMBxao6p/ikntrYnwNGQyGOJBflh9SAkECBMgvy29RuuFuqCdNmsSkSZNC10E31Dk5OXz++efs2rWLP//5zxw8eLBFeQZx5GtIVd8B3olLjgaDwdCO+dXpv6I0hhvq5V8u538//l8qaipCYSnuFH4z8Td8f9j3m52vEzfUQfr168eIESP48MMPueyyy5qdZ5AGewQiUioiJQ0dLc7ZYDAYOiCXnnApZ+acSRe3Za/v4urClAFTWqQEoHE31Hl5eVRUWMrnyJEj/Pvf/2b48OEtfR0ghiJQ1QxVzQT+CNwB9AdysKaS/j4uubcFZozAYDC0kLsn381xKcchCFmpWfxu8u9anGZjbqi3b9/O2WefzejRoznrrLO47bbbGDVqVIvzBWemoWmqOiHs+nERWQfcF+shERkA/AXoAwSARar6UJ04AjwETAfKgRmq+mkT5HeOGSMwGAxxIs2TxmPnPMZtH9zG/WfeT5onrcVpNuaG+txzz2XNmjUJcUPtRBH4ReQ/gGVYU0ivBvwOnqsBfqGqn4pIBrBBRN5R1W1hcS4ATrCPCcDj9qfBYDC0a4b1GMZLl7zU1mLEBSfrCH4EXAEctI/L7bCYqOr+YOteVUuB7VjmpXAuAf6iFmuxpqj2bYL8BoPBYGghjfYIVHUPVoXdbERkMDAWWFfnVn9gb9h1nh22v87zs4BZAL1792bVqlVNliHzwAFSgR07duBrxvOJpKysrFnv1Bq0V9mMXE3DyBWbbt26UVpaGrr2+/0R1+0Fp3L5fL4mlWujikBEniGKqzZVvS5K9GjPd8Vag3CrqtadbRTNcB8tr0XAIoDx48frlClTnGQdwb633+EocNJJw+nejOcTyapVq2jOO7UG7VU2I1fTMHLFZvv27RG291jTR9sSp3KlpKQwduxYx+k6GSN4LTx9LN9DjhxciIgHSwn8TVX/GSVKHjAg7DrHadoGg8FgiA9OTEP/CL8WkWeBfzX2nD0j6Clgu6o+2EC0V4C5IrIMa5D4qKrubyCuwWAwGBJAczamOQEY6CDeGcBPgLNFZKN9TBeR2SIy246zAvgK2IXlv+jGZsgTk6IlS/CujRya8K5dR9GSJfHOymAwGJqFEzfUDz5otadLSkro378/c+fOjVv+TvYjKK2zovhVrEVlMVHVj1RVVPVUVR1jHytU9QlVfcKOo6p6k6oOVdVRqrq+5a8UScrIUeTPn0/NoUMAVO7eTf78+aSMjM9CDIPB0LlIROPSiRvqM844A4Df/va3nHXWWc3OKxpOnM5lqGpm2HFiXXNRe8a3ZTNZs2ZRvs764oqXPUfWrFn4tmxuY8kMBkNHJNi4DCoD79p1LW5cOnVD/dlnn3Hw4EHOO++8uLxLECezht5V1XMaC2uvBL+0pN69qd67l+QTTqBo0SL6L1zY1qIZDIZ2yIH/+R+8W7ZyuAE31ABJvXqRO3MmSb16UVNQQJehQyl89FEKH300avwuJ59En//8zwbTc+KGOikpid/85jf8/e9/5913323xe4YTy+lciogcB2SLSA8ROc4+BgPxcYLdChQ9/RQpY8ZQvddaruDbtImUMWMoevqpNpbMYDB0VNyZmZYS2LePpF69cGdmtjjNxtxQP/bYY5x33nkMGDCg8cSaSKwewc+AW7Eq/XD/PyVAdLXXDvH060/xsmW4uncnUFyMu19fvCtX0v2qq9paNIPB0A7p85//2eh8/aA5KPvGORx5dhnZN91E+sSWecdpzA31smXL+OCDD3jqqacoKyujqqqKrl27cs8997QoX4ihCGwHcQ+JyM0dciMam+Sc/qRPnYp35UoA/Pv2kz51Ksk5db1dGAwGQ+MElUD/hQtJnziBtNMnRFw3lzPOOIMHHniA448/PsIN9datW1m8eDEXXXRRSEEtXbqU9evXx0UJQGzT0Nn2ab6I/KDuEZfcW4GUkaPwbdyIu08fAJL698e3caOZNWQwGJqFb8vmiEo/feIE+i9c2OIJKI25oU4ksUxDZwHvARdFuadAtJXC7Q7fls1kTJtG8bJlANTs20f3K6/Et2Vzi7tyBoOh85EVNr8/SPrECS2uTxpzQx3OjBkzmDFjRovyCyeWaehO+/PauOXWBnjXb8C7ahXdLr+coy+8QI8f/Ygjf/sb6VOmRP1CDQaDobPhZPpoF+CHwODw+Kp6d+LEih/+w0VIairJgwYBkDx4MJKaiv9wURtLZjAYDO0DJ07nXgaOAhuAysSKE3+GPP883rXryLOXYx96+GEGPP64MQsZDIYIVBU5BnYy1GZsx+tEEeSo6vlNF6f9kD5xAl2nTKHktdfI+O53jRIwGAwRpKSkUFRURFZWVodWBqpKUVERKSkpTXrOiSJYLSKjVLXD+mTwrl1Hmb1JQ+m//oX3kkuMMjAYDCFycnLIy8vjkO2TzOfzNbkybQ2cyJWSkkJOTk6T0nWiCL4NzBCRr7FMQ4LlL+7UJuXURgTn/GbNmc2hP9xPz3k3x2XOr8FgOHbweDwMGTIkdL1q1aombezSWiRKLieK4IK459qK+LZsJuPcc0PXKSNG0H/hQkpWvG45pDMzhwwGQyfHyX4EpVGODrWLWPKQIRQ9/kTo2rd9O0dffc0sKjMYDAacKYJPgUPAF8CX9vnXIvKpiIxLpHDxIGXkKIoWLSJtgmUGOvD7/0fBfffRc948YxoyGAwGnCmCN4HpqpqtqllYpqLnsXYTeyyRwsWD9IkTyDj33NBgceW2bWRedBEpJ59sdikzGAwGnCmC8ar6VvBCVd8GzlTVtUCXhEkWJ4qWLCF5yBAIBACQ1FRK33mHvT+bZUxDBoPBgDNFcFhEfiUig+zjduCIiLiBQILlazEpI0dR8MADYC+y0IoK61AoWbGijaUzGAyGtseJIvgRkAO8hLXKeKAd5gauSJhkicTtRtxuLN95BoPB0LlxsmdxoarerKpj7Q3o56rqIVWtUtVdrSFkS/Bt2czAJUtw9ehhBSQlQSBA2oTT6Xt3h3CXZDAYDAmlUUUgIj1F5A8iskJE3gserSFcPMiaOZOipUsJHDliBdTUkDZpEhUbPmX/ggVtK5zBYDC0A5yYhv4G7ACGAL8D9gCfJFCmuOJdu47y1avB9h/iysykfPVqApXBRdIGg8HQuXGiCLJU9SmgWlXfV9XrgImNPdRe8G3ZTM/588HtBkCSkpDUVEQgc/r0NpbOYDAY2h4niqDa/twvIheKyFisweMOQdbMmeCvIXX0aAD8hw+Tde0MBjy5qMVbyxkMBsOxgBNF8HsR6Qb8ArgNWALMT6hUcSZr5sxa01CPHhx5dlltuMFgMHRyGnU6p6qv2adHgamJFSf+FC1ZAu4kfJs2AZB03HF0v+wy9s6ZYzaoMRgMBmIoAhF5ONaDqjov/uLEn5SRo9g7Zw6e44+naudOAhUVFC1aRM9588wG9gaDwUDsHsFsYAuWX6F9dNApNukTJzDg8cfZe8MNANQcOMDAp582CsBgMBhsYimCvsDlwJVADfAc8A9VPdIagsWT9IkT8AwaSNWu3bgyMowSMBgMhjAaHCxW1SJVfUJVpwIzgO7AVhH5iZOEReRpESkQkS0N3J8iIkdFZKN9JGx1l3ftOqpz9wIQKC3Fu3ZdorIyGAyGDoeTlcWnAbcCPwbeADY4THsp0Nim9x/abivGqGpC/D0Et6pMtbd3S+rZk/z5840yMBgMBpsGFYGI/E5ENgA/B97Hckd9vapuc5Kwqn4AHI6PmM3Ht2Uz/RcuJKlnTwAkpQv9Fy40awgMBoPBRlSje+AUkQDwFVBhBwUjOt68XkQGA6+p6sgo96YA/wDysAajb1PVrQ2kMwuYBdC7d+9xy5YtayzrCNLeepvk7dvpsmMHNdnZFP3+v/Hs3IlnzzeUTzuvSWklgrKyMrp27drWYkSlvcpm5GoaRq6mcSzKNXXq1A2qOj7qTVWNegCDYh0NPVcnjcHAlgbuZQJd7fPpwJdO0hw3bpw2lbI1a3X7qFN12/CT9Mvvnqtla9bqzomTtGzN2ianlQhWrlzZ1iI0SHuVzcjVNIxcTeNYlAtYrw3Uq7EGi7+JdTRLJUWmX6KqZfb5CsAjItktTbcuubNm4du+ndTx1vbK/uJia13BgBwze8hgMBhw5mIiIYhIHxHL74OInG7LUhTvfNInTabgvvsIHC0BrFlDWlFB5gXG4ZzBYDBAAhWBiDwLrAGGi0ieiFwvIrNFZLYd5TJgi4hsAh4GrrK7L3El69oZpJ91Fr6ttcMPkpoKYDavNxgMBpxNH73FSVhdVPVqVe2rqh5VzVHVp9Ral/CEff8RVR2hqqNVdaKqrm7eKzRO+oQwE5DHQ8958yi47z5wN+pqyWAwGI55nPQIrokSNiPOciQU77qwNQPV1Rx6+GF63X47+GvaTiiDwWBoJ8RyOnc11ib1Q0TklbBbGSTAlp8oip5ZinfVKjxDhlD99dcAaEUFVd98Q3JO/zaWzmAwGNqeWLaR1cB+IBt4ICy8FPg8kULFE++a1aSMHo1vi+XpQtLS0Joaip9/nu6X/bCNpTMYDIa2p0FFYE8R/QaY1HrixJ+BixaRO3sO+P1WgN+PuN1oVRXVBYfaVjiDwWBoBzgZLP6BiHxpO4grEZFSESlpDeHihadXT3BZr6qVlWhNDZKaaoUbDAZDJ8fJYPF9wMWq2k1VM1U1Q1UzEy1YPOl79914+vWrDaiupue8efS9OyF+7gwGg6FD4UQRHFTV7QmXJIHsX7CA6n37agM8Hg49/DD7FyTM87XBYDB0GJxMpF8vIs8BLwGVwUBV/WeihIo33vUbIBAIXUtSElpRYYUbDAZDJ8eJIsgEyoFwN50KdBhFUI/4L2A2GAyGDkujikBVr20NQRJJ+vhxFO/ZE+oVqN+PpKaSbjuiMxgMhs6Mk1lDJ4rIu8EtJ0XkVBH5r8SLFj/63n03npyc2gAzWGwwGAwhnAwWLwZ+DVQDqOrnwFWJFCreeNeuozovrzYgOZnCRx8121UaDAYDzhRBmqp+XCesQznpKXjwgYhxAVeXLgSqqqxwg8Fg6OQ4UQSFIjIUe6tKEbkMy/VEh6Hyiy8jFEH2jTdCVZUVbjAYDJ0cJ4rgJuBJ4CQRyQduBeYkUqh402X48IjrggcfjBpuMBgMnREns4a+Ar4rIumAS1VLEy9WfMk897tU5eYSOHLECqiuJn3qVNLHnda2ghkMBkM7IJYb6h+r6v+JyM/rhAOgqg8mWLa4UZW7l8DR4oiw8jVr8PQ0voYMBoMhlmko3f7MaODoMHjXr4dA5CIy9fms8A5K0ZIl9WY9edeuM9tvGgyGJhPLDfWT9ufvWk+cxBDwVUQGiIBq/fAORMrIUeTPn0//hQtJnzgB79p1oWuDwWBoCrFMQw/HelBV58VfnMTg6tIFf1oalJdbAap4hg6FgL9tBWsB6RMn0OeuO8mdMYPMC6fjXb0mpBScULRkCSkjR0XE965dh2/LZrJmzkyU2AaDoR0SyzS0wT5SgNOAL+1jDNChatC+d94FlZW1AW43/oMHrfAOjCQnA1Dy+gp6XH2VYyUAtT2KoHkp2KNIGTkqIbIaDIb2SyzT0J8BRGQGMFVVq+3rJ4C3W0W6OFHw4AMR3kfx+wlUVlLw4AMMef75thOshfh27gTAM3AgR55dRtrpExwrg/SJE+i/cCG5M2aQMmoU1Xl5TepRGAyGYwcn6wj6ETk43NUO6ziIK2JBWferroLqaiu8ndDUwV/v2nUcXvIUAF2GDKH/woURLXwnBCt93+bNTe5RGGJjBvMNHQknNeE9wGcislRElgKfAv+TUKniTFVuLpJZu6la6VtvkTJmDFW5uW0oVSRNNdX4tmzmuOuvty5EQi1835bNjvMMr6iOPLvM+F6KI8b0ZuhINKoIVPUZYAKw3D4mBc1GHYXsWbPQktptlrucfDK+TZvInjWrDaWKJFiR5910EwfvuSdiRlA0smbOJKXOyuj0iRMcD/QGK6YgzelROKGztozDv8/d37uo0e/TYGhLnNpGKrH8Cx0BThSRMxMnUgLw1+DKzg5dlq9eTfqUKeBvX77zugw/kYDXy+Glf3Zoqmn+Bju+LZsjppo2p0fhhGDLuORf/0KrqztVyzh94gQCXi9Vu3YZ05uhXdOoiwkRmQncAuQAG4GJwBrg7IRKFk/cSQQKC0OXSTk5eFetIv3009tQqPqUf/IJAJKa2rTBX3u1d1OI1nNIn+h8sNkp6RMn0O/BB9h77XUkDxuGv6io07SM65remjKYbzC0Jk56BLcA3wK+UdWpwFjgUEKlijPeNatxdesWuq7JyyNl9Gi8a1a3oVSReNeu48BvFwDgSktLmKmmLQgq3GO5ZVzXBOZdu468m28OXR9L36fh2MOJIvCpqg9ARLqo6g6gQ7nt9PTrT+Do0dB1lxEj8G3ciKdf/zaUKhLfls30udtexO108LeFey9Hs9Mnwn7vXVe7nUV7GpSO5/hF0ATmsaf0lqxYUe/7Cf8+O8M4iaHj4EQR5IlId+Al4B0ReRnYl0ih4k31vnzcYQ7mKrdtI33qVKr35behVE0jWqXl++IL66SOaahu3KIlSyh6ZmlExeNdu67erKlE2O+9a9ex7xe/CMnZnlrG8ZzZE1Tc3RctJnfmDZS+8w45jzwSuh8cmM+aObNTjZMYOgZOZg1dqqrFqnoX8FvgKeD7jT0nIk+LSEFwr+Mo90VEHhaRXSLyuYgkzCe0v/go/qKi0HXmRRdRvmYN/uKjMZ5qXVJGjmL/gjtD13Uri2iV1uGnnm4wrfz58yl5+x1UFdxJFNx3H4GqKrSqKpR25vQLQ898dcn3yZ8/n4xzz62XXktar74tm+kXthNcSwal4z0DKShL/q23sv/OO1s8syd94gT83TLxfvQR6WeeGZFOcAZRwQMPmhlEhnZHTEUgIq7wilxV31fVV1S1ykHaS4HzY9y/ADjBPmYBjztIs1n4y0ojVhaXvv026vNZ4WHEq6KJls7+BXeyf8GCBtNOnziBPr+7y5K3sLBeZRGajjh3Lvt/dzf58+dz3HXXWgnV6RGkT5xAr9t/Sf68eey9YRZFixbR6/bbKXz4T+y+5JKoFVHlzp2kjR9P5vTpEQrHs3Nni1qvWTNnkj4hssJryjTXcBIxNz994gT8FRUUP/d8i8cvvGvXkXSwAICyd9+N+A240lIJeL0ULV58zI6TGDouMRWBqgaATSIysKkJq+oHwOEYUS4B/qIWa4HuItK3qfk4IXngoEjZfL6o4dEqmr1z5oA7cnJVY8ohWjolK1ZQ8sabMSuxtNNqO0XRKov0iRMIlJVR/Oyz9Lj6KlJOPLFBGdzdulv5fPQRPa6+iqxrZwBQ/fUe0s/8TtSKyLvaGjwPKpwDd99N98VL2k3rNaQM582j6wsvxKVl7V27LuSHKtr4hdPGQfD7rM6xxp2yfjYrYp2G154R5snp367GSQwGcDZG0BfYKiLvisgrwSMOefcH9oZd59lhcadyx47IVrPLBUlJVngY4a3ug/fdR/78+fScN4+iRYsct0KDFUT/hQvJvfZavjjrLPJuvpnM6ReQ86c/kX/rreTdPC9qJVa+4dPQebTKou50xKCvoWiE/BANGsSRZ5dR9MzS0L2SV16l6Jml9dLv8dOfhCqvQFkZR/7+LOV1TBxNoV4lqtriQdL0iRMIlJSQ/u57cWnBN7aoLqjUy1a9j7+kpMHvP7guQ9O7Ws+ddFLEOo2ix58AIHngwBaPk3TWRXqGxNHoOgIgUfsRRJv8HnUajIjMwjIf0bt3b1atWtWkjLplZ9PlwIFQhhoIQCBAWXZ2vbSkrIxeZWUcfvoZyqZPZ9+QwXhmXEP1jTdS07MnSQcPUjxnNvt8FbBqFZ6dO/Hs+YbyaecB4PH76T53Lr4xY0hTxX+wAPV4+Kp/f9i0kW7V1fjfeYfyM8/kEzuNsrIy/v3kk3RftDikmQ/NuIbquXMpvmEm1cOH49m5k+6Ll0Tef/wJXEBhYSG77fdIe+tt1O2i62uv4wLKMjOoGTSQmnvvDb1/yQ9/gN57L4Hu3XCHvfvuqioCM66hdMkS0uyw1Pff599PPkl1M/Z3DpXF6NGh9PbY78STT0aUm+M0d+7kOPu8YMlTfFVQQMV3vxtxP/3td/Ced26EzHW/J7DKqnrGNRy38I8AfOKrwDPjGopffpnysL0qPDOuoWb2bAQIdO1K8Q0zQ99/iGHDSHv5ZTyl1gr2zzdtomrkSHrbt4+eey4Zy5dz+PARvmogH8dlYJfr0Wt+StXIkXi++ILujz5G2cUXsTlMpvB3Lisra/L/pjUwcjWNhMmlqgk7gMHAlgbuPQlcHXa9E+jbWJrjxo3TprJzyhTdNvykesfOKVPqxS1e8YZ1f+Qo3TFuvJatWauqWvvciJGhsLI1a3XnxEmh69o0VkTks2P8t/Sbn83W7aPH6LaTT7HCvnW6Fj79jBYuXqwrV67UwsWLa/MeflIo/cLFi1VVtXDxYi1bszbifsEjj+i24Sdp7k03hfIuW7NWt40YqXtm3qDbhp+kX11+hW4fM1a/mT0n4tlo5XHwj38MvVMwbP31M+u9Y1CWcMJlrRu+ffSYUHpH33qrwXJrjLqy7bvzLt120sla+PQzllxPP6Pbx4zVwqefiUi/sfzCy6WhdwvGKXjooZjybT1lhG4bfpKWrloV8VzpRx/ptuEn6TfXXtukd26I4tdft9K77nrdOXFS6J1L3ntP/T5fvXdeuXJlXPKNB+FlHJSrod9PW9GS8mrK/6OptEQuYL02UK82aBoSketF5Jdh13kiUiIipSIyJw466BXgp/bsoYnAUVXdH4d06xEo8zoK965dx4G77gLAlZ4OIuTNnRthVhGPh71z5vD15Vc0aKOuzouclpo2fjzeVatQnw93T8vVRbdLL6XgvvtC4w9ZM2eSdtrYiOeC6XrXrrMGXcPy2b9gAV1OOCFC9qIlS6yB4l/8gvIPPwTAt307PefNw7dxY0TcaBz5y18pWbEiwqRR06d3vfnvVbm5jgdt0ydOCI3JAOTPu4W8m2+Oadtv0PTx9FMRspW+9RbdrricggceIP+2X1Jw3330nDePrGtnWCa+m2/m4P9G+m2KlnY40cZ38ubOrS2jMJNd3bTSJ06gxnZlUrhoUYTZKd4kZVn5eP/979AYUP+FC8mbcyO7zvluu56ZdKw75OuI7xdrjGA2ED4/8ZCqZgI9gasbS1hEnsVyRTHcViLXi8hsEZltR1kBfAXsAhYDNzbnBZyQNi76zNS64b4tm+kTVAQeDzl/+hOB6moKHqid/jjg8cfRioqYrpvLP/444rps5Uo8Q4aQPuUs/PaskqP//Ce9br+9UX9HdX9UQUreeJPKL74EwH/kSMSgdnBgGIDqaooWLYqoQBuqoHr85CccfeUV8m+7LRTW9c23QudFzywNTTvtd+895M6YwVcNzEIKVpLRKt3UMaMpevqpCAVbtGQJ++/6HbmzZlGVm0vezTeH1j4EV+l6+vSJyKPfgw9w9MV/QE0NJa+9RuZFF4XePX3iBAKlpRz+859JP/M7+LZsxrt2HSkjR5F3883sX7AA79p19WZyhU8pzZ15g6UEwsaX+i9cyN45cyh6ZmnEdxNMK8l2ZVKx4VN6XH1V1HKOB75t2wBI6tcvpJyCZeMvLGzXM5PCHfJ1ffEf7VppNYfwSQ35v/xlh3i/WIrApapFYdcvAKi1yji1sYRV9WpV7auqHlXNUdWnVPUJVX3Cvq+qepOqDlXVUara5jvJW63yWuWQPnGCNaOkJnpl3dCAbsVnn9WLW3PgAFkzrg1dZ5x3HlnXzghNoyxasoTyTz+NeCa4dWT/hQvJu/FGvrr0B6F7OX/6U2hwsOKzjaRNmEDhY4/hXbuOr6+4MiKd9O98B9/27bXv2YDX1aq9e61ptWF+mcrOn8be2bPx7dhJwX33kTVrltXyteNU7vwiaqWTMnIUeXPnRrhZCFL+8Sd4+vWn4L77QsqgKi+f4mXL8PTrT+b0C9GaGgruvZcjL75oVcaqEeseAMrXrAG/tVleUp8+lK1cyf4FC0IL6IKUvPIq3g2fsnfOHKscVCl+4UVyZ86k5I0368mXPnEC7uxsvB99hGfgQHL+9KeIez3nzePQw9ZOrkHFkDtrFiVvvBnqEYD1+0gE3rXrKHzsMQCSc3JCg8/h79zeZyYlDxlCwOsl/V//atdKq6kEGy7BSQ0lr75m/f/i7Mwx3sRSBN3CL1T1f8BaWwBkJVKoeFO5Y6c1UygcESvcJmhaOfL3v0eE1aXuLJNg6zCIb8tmMi64oN5zXY4/PsLEUPLGGxGt0ZSRozhwV+24fPjU1fSJEwiUl1MZVplHmFySk8maMQOtrCT3hhvwff55RN4lr7xCwb33hq6LFi2qJ59VJPXH77u++Rbq81kt7osvouD++/ly6tkc+P3/s57p2jWi0gk3UWVOn24NzNcti1NOsc06V1Bw771sP2UExcuW0f2qqyh5/XVyZ84MuWeo3vMNgaoqMqdPr5dOUdiCupqiIrSmhpI33qQqL98yu9l0v/JKvCtXktSrFwX33WeZ/VShpoaUESNq07MVq3ftOqr27LHyr7P6umjJElJOPpkBjz9O3rx5FD3zDFpRAVVVdD17aqhHAET0wnx1Zqi1BN+WzWTfaFtnbXckWbNmhZRTMO/2soI7GuWfWL3mgO1gsb3K2VSCvcSIhsirr9abgt7eiKUI3haR30cJv5sOtlVlUu/ekVtVuqwdyyQ9HYi04XU56SQAakpLo5pQwlvT4a3D4A85ZeQoSt94o95zlbt3o/6wrZ5VI9YVBDejD5I/fz5pEyZwaOFC9t9Vf+LW7osvqb3w+ShaupRuP/iBtfNaNDye0GnKqadGjVL63nv1wirGjLFOXC7K3lsJfj81+/eHWuJaXk7WrFmhH3+4icrdMxv1Rh+fyTj3uyT3s5eNBAKQlETm+eeTlJ0NNTVo+HsEAiQPGVK/YgsrTxEBVVLHjaN6X75ldrMpWbECgOpvvoGkJGoOHgzd823dWlsuI0fV/haGW2s0smbPjvgdBP/oAIGSErzvv19bfm+/Q03P2h5BOMHpo1HvOXQJErzOmjmTFPt3GjJb+WsY8HjtmsyWuhVP5BRV79p1HPhvq2rRY83B4sQJHDfrhoiGV6/bb4+Ygt4eiaUIfgkMtV1A/MM+dgHDgNtiPNfuqDl4EJJqJ0pKcjK4XFR/9RX777wrwoYXMg35fCFFEUGYTT+8dbh31iy+ueYa8m6+GXd2/cogedgwNMzE1OuXt5Hzpz9F/FHTgpUu1oKyrBkzUFWKl9U3MVQF/QwB7uwsvKtWUfzCC1HfP/nEExm4eHHouu4YRhAtL68XlrZmjXUiElk5u63yTOrVi0MPPkiXk07i0MMPR667qI5uUqvcts0y8ySFtZJqasi94Qaqvv7aug5XmjU1HHr4YTKmTWNvuFkrTLl5Bg1CPB48vXoycNGiiHGSoHKX1NR6ijI1bIA+feKEkCnObXurTTk5cj1AaK/nG26o914DHn+cpEO1PYJws1jWnNn14gcJKpf9C+60ys12CYI7KTT2UG+wMdhj2rcv6mSCYK+sOSu4w2VKxICnb8tm+vz2v0LXidoLo62o2yMPDuS35/drUBGoqldVrwbOw3IXsRSYpqpXqWpZ64gXH7qe+R1cKbXDGimjTw2ZB4qfey7SRhlmHanJy4uZbnjrUKuqKF/3MVpdTVqUfQ6q9+yJqIRSR44M/VE9O3daYwRhYwuH//JXANLPOKPR90vqYc+sD688w6j64gvyf/3r0LU2MOYRlWCafj8SVvEG/8hqt97L16yxxjzsH/3eG2/k8NKlUZNM//a3KVr6DIfurx2ETxk9OrKSDjNTdT33XLSiguLnnkOrar2bdD3rrNp3/PJLMi+8kOSBA+sNUlfYik8r6s/Z977/QcR1sELVMM+hUe3XUXpeJStWUBPm3DBQWuvCJNiC9x89Wq9VHawIj77yCnt/9jMKH32UXrffzqGFC8m97jpK3niz3mBjUD5X1/SoremUkaNa1KoPH/A8eO99cR3wzJo5k9SwRk8wv4aUVvh7hE9CCDfltafFdC57UWGQ4JhBc5Vya+DE6dxXqvqqfexuDaHiTfLAgXQ9r9aZWsW6j0kdPz503SQbZZitL/hn2TundjateDwkD6zvkcNdx2Rw4N772L9gAV9fcQXdH3+Cqrx89v/nb0L3A+Xl5F53HeXrGper5siRRmR2498fNjO3AYURFVdthRw+aJo6erQlZ9D043JFzF7R8vKISjucsvffpyp3Lz1v+0WD2XY55ZTa+EGTVR23zl57iiyAKzub4ueeo+Sdf5E7axa5wf2coVapuN04oWjJEvy22/LSN9+s99vYf/fdUZ/z7dhO0qHoW3VUbLFcdvl2flGvVR2sxNTnQysrCVRUUHDvvVb5BQJ0nTq1fmvSLoqk7t2tCvuWWzgYplghslXv2bkzes8iBsEBz8PPPNOmA7rh7xGc9ZU3d26kKa+dTM30rl1HyeuvR4R1BLNX+x7BiBPe9Rvwhq3GSxowAO/KlaHroI0ya9YsfNu2RkmhlsJHH4249m3fHtHSzPnTnyIGhYNUf70n8rkNG/B9+qllshIXR//5T467/rpaW7I9ptFQZRpO+Cyf6BHqVPxN2sdAiL7g26pc1fbTA5HlGAtXt25Uf/11hJLzbdqEKyuLgO0ltjLMdt+Q4gofiA4UFpI+dYrVwq87QO3xQFVVowqw6JmllunPnRSaSJA8eEiEiafomaVUf/VV1OczL5hOSVkZyV99Xe9eob16OTj2EKz8U0aOClVuIerIWfLqq6Tcfju5s2aRPmky+Gvwfmz5Lqrat5+ip58icPQoh8Naxfnz59N1yhS6nnMOudddRw8RStLSyL7xRnxbNjuq1Ou6NAnfYa1oyRJSRo6KSCc4yy3eLd9w1y/dr7gCVAn4fBQ++SSVO3a0q6mZvi2bybzwQo6++GIoLGgWai8yRsPpnsUdmvL16yNmDdXs3RtxP33iBDKmTaPgwQfw7doVM61AWMWXO3s2h8LsxwBFS58ha3bD9uAIVK06VhX1+ylbuap+HCet96ZuVdkURRBWqYbb5w89avvad9eWq2/LZlw9elBw//2xkywqsvwOfVDbonf36xdSAkCztt/0rlxVXwlA7QybRihYuJCSd/5FwYMP4MrMBODoyy9HmNIKHnyQXr/6VdTnG5qNVZdgC7a2pbs25neSPmUKBQ8+EJpyW/LOv6xnsE2OdWfEYVU+R5cvtyqkQADx++l69lSKFi2K6Ser7phAeHrhLduUkaMiZsyF4ruTEmKm6XLCMAJlZRx++mmO++lPoKaG8jVr2t3U06yZM0keMCAirL2bhcChIhCRb4vItfZ5TxEZklix4ktS715RK4gguTfMovi55+j181+Qec45sRMLsw17V71fz9zgff8DS/E4paYG8fvB76/nBM8pUQe1E0B4hVj6ujUTJzQgHAiQMnIU7oyMppmebPz76ux15ERZRakAW0R1Nb5Nm6C6xppQAFR+8UWkLDHeLf3MM6P2BsLxbdtO/4ULKXr6KWvV989/TtETT+CJYk4M4l21iu4/vIzknP5k3zzXkjGsp+hd9X69Zw7/7f/qhZW88mpoHUg0wk0wvi2b682Qyzj3XEpWvB66zp53MwX33svu718a6gnGUjRRcdgo8dq9R0lPD42fQftfL9FRaPSfJCJ3Ar8CgqONHqD+r6wdkz5ufMxKw/vhh6RNOJ2qr7/iyF8bebWUlIjLugOQ2bfeEmGGckQMJeUELWulsftG5Cx84gmqc3NJnzql8bTiUImnhq0BiEXhI482HglClZIrIwN/QUFtcPh7u90RUwPDqdi0yVEeJStW4D9aQsF994VmcFXaK4WjkXLqKPredWfU1mZDlL3zr3phmRddVG8aY3gvIGSCuXkuRX/9v3pm0OQhQzj66mu1z/utcqnasQPPgAGhFezOWujOe3zetes48Dt7XEYkoreYNWsWe+fMadfKoCN4i3Xyb7wUuBjwAqjqPiAjkULFG9+OHY22PMrXruPoK6+S1K9fI4n5Yt72ZPeMef9YpnztWvxlZXg/+nfjkVuo/ACqw9YDxJPw2T5AROs77VvfalgeexFaTFwuqgsOUrV7N7jd1mKjRqjO3RuaKVPYwK50Tih95x1rHCyG3yiAQIUP/8GDEeNT3rXrKFq0iJ7z5tkD0/dTGLYVp+/zz0n/TvR9LpwQq7L0bdlMnwW/BUC9XrJvrPVGE5SpPU/NbM5U3IbKI+2txCzhcjJYXKWqKiIKICKtY4eII9WHChx1QdXvb3FL9WADrcVYNN0a3no0WbaGFrQlgJp8Z3tOewbk1Busby7l9uY9zcXVLZOK9RvI/N73oq4PiUbGtGmUrHido6+8GuHAr8kE/BQ++iip48fjXb8e36bP6b9wIZnTLyRv7lzcWVnWlq52izt8Cm1oEoC/xh6YfipiOnFQqflLSsiaMSPqIDJQb4CZ6urQwHP4FNVgZRm8rg6aDl2uiB5ZcweK22Swe948Uk4+mcovvmhU7vDy8G3ZbI29LFpE9YxrEiKrk1rveRF5EmsHsRuAf2E5ieswBB29NUogQKCFZpZASUmLnjfEn3gpgXgQOFpCoLKywcV/0Shetozify4nqV8/Mr/3vWbnPeDJRQTKy/GuWoX3/Q+stRtYM98CZWVUf/MNAZ8Pz8D65qcuJ59M0aJFVIV51g1vWklSEp4hg/F+9BF5N99cO+/fXm0eOTgetoVnWVmoQramYs9m90UX11+3EFRKdcZoGvIo25jppbU9hKZPnGCtt1m3jm4/+EGjyis4RpM/fz7ln2+m4L77yJg2Dc+ebxIia6M9AlW9X0TOBUqA4cACVX0nbhK0Bg5noEhysuWGwGBIFPamSE1FunQhcOQInsZMlzEofOKJUN6ubt3w/vvfteNZLpd1r6YGd0YGdft15atXk3ziiRQ/91worMuwYSH/V1pZSY/Lr6DwscdIHXcae2fNwt2zJzX79tHr9ttDFV//hQvJnTEjNMEh0LVrqAdQsuJ1tMJH1ZdfkjZpoqN3ssxbeyl84klyHnkklFbe3LkkH398zFZ/0Mts8rBhVO7YQc4jj4R6Lp6dOynatYusmTPj0vr2rl0XGk8sXraMrmFmtGg9pqCi0kCAsrffJm3yZIqfe46kb41PiDdTR3YQVX1HVX+pqrd1OCUApIw4pfFIRF95ajC0B9Tnw9W9O0VPRxkjcGjOLLennAIEiosjzXhhysm3cVO9MLBWqIcv9Au5AwFwu0k5+WQyLzjfWmFfVUVNfn6Ea/DwCi/cB5V37TpyZ82i+MV/hMIqPtvI3lmzQtNTj7xQOy8/nPz580keMgRE2Dt7NvsX3BlyHZ55wQXsnTOHwqeeRu1tUsOnuKZPnIC/uJiK9esJVFTg2749tKaj+6OP4V2/vsmL8KIRdKMewt7nJDjuU3868brQlPaAvbCxfM0ay5fWx58kZMqsaCO2cxEppf6KoqPAeuAXqhp9dU2CGD9+vK5vyvRMYOe3vkWgtEN5xTAYnGM73GsSwR5ArCjZ2QRiLVasm29yMklZx1Gz/0BtGl27kn3TTVR9/RVHX32Nbpdc0uDYiHTpElqgKF2S0eoaCATwDB1K9e7oTg26X3E5yQMHkjJyFLkzZljPpqTQ7eKLyZw+Hd/27RTcey+egQPxHz5M6vhxobERIPRMkPTvfIfyTz4h4PMhtvzhPQ0nPYPCJxdFrC9KyskhUFyMVlejlZXkPPE4+fNuwZWRAYFAROs+6HU4ffIky8mjXb4po0bh27wZf3o6ycnJzeoRiMgGVR0f7Z6TpsSDWA7o+gM5WA7nFgPLiNy4pt0SqGn6vHaDocPQVCUAjsxTMZVAtHyrqiKUAECgrIyCe++tVQJhpqVw3L17M+DJJ0PXaZMmh9boNKQEADKnXxi1Yg56qw32WqpzcwmUlVGx4VOyZs2iZMXrEQvmgosEvR9+GDEg77Y3Q8qdbe05Eb5gbv+CO/n6iisixie8a9dR+k6k0aQmL48eV1+N2O+TNv5baFUV/qKieq37LsNPRCsqKHv3PTzHHx8K99kuStRWAvF2W+FEEZyvqk+qaqmqlqjqImC6qj4H9IibJAlEkjqFJw2DIeF0OeXkZj3nzs623Lc04O/Jf/AguWE+uzy9eja4IVQ44WMCQcTjofDRR8maNYvil16KiJ8yapS9AlwivMqmnBz9vap37WLnt79jjaWoUvjooyEfR8X//Ce+rdvImzuXsrVrQ6358E2gghx59u8hN/ThC06PPLus1uts+D23O0IB9vjxfwCgXZIT4q3ViSIIiMgVIuKyjyvC7jWjKdL6GNu/wdB0wlukQSobccHSEDV79+LbsZNep1U2HCnsf1p9qNBxT6dkxesEwlrxqePGoX4/JW++EbEGBKxB74xp0+h7d+QeH3tj+McKFBZaZrDqarSmhqKnn7Zs/vbitkBZGXuvn0nuddehFRUNeAyWkNlrX9hWsP0XLqRkxYrQ1qwHfmtvVuX3kxrmxTi4MZNUWu8Tb7cVThTBfwA/AQqAg/b5j0UkFajvXa0dEjHf2WAwOCKqc72qFqwTqaqi6HOHLiVWrkRsf0+xsMw0kbMCK9avh0AA36bP68X3DB5M8QsvkDt7doTX4LpjpfXmGapCUhLq8+H94AMCPh+ulBRSx9r7Wfj9lrnN46Gqji8zgIwLzq9NKmyQPriTX+q40yh44AHcfXpb+aekkNSjvsEl0L17lFJoOU7dUF+kqtmq2tM+36WqFar6UUKkijPhHjINBkPb4fc5NNO6XKiDNTmfPbaUvS+v4OBVta3jtAkTGvzPV+/ZA34/3g8+pNsltbv8eQbkNC5TuKmquprsG28M2f1DqFL9dX1/U0fDZj1pnUWXmdOnW0qrpoYq2+ttoLKSF/Jq8/vgi0bGa1pIo9+KiKQA1wMjgJCjHVW9LoFyxRcHMyQMBkP7wdWjR6Q32gboX17E34d/lxcrh/NPO6zs3Xcbz0CVoy+/HLp0tOjQ7a5d0OZyUfDAA/WdEDrZ9ClsXVPRlf1JOa6KjONSKD5Su6FNNcJ2z3EE+xGPv7+bexpPudk4MQ39FegDTAPex5o5VBrziXZGLM+OBoOh/eFECQS58Os1nLBvZ9MyUI0cO3QyHhFe6Qf8UFPdrBlbaRPCFrgVeMhdlUXx7kjPPR4NcFpB7TtVVdfmnQiHdU4UwTBV/S3gVdU/AxcC7WM7IIf4G9vBy2AwdFiWnXA2d615qnUzdQUVQNMVQcWnn4bOvftToiYhwFkHar3Z/s37eyvc5yNv7lyqcuuPQ7QEJ4ogaNAqFpGRQDdgcFylSDDB1XkGg+HY44atr9Il0HrODgHcyUp6Px/NcRmpleVhVxJ2RJLUpbYXcHCjNXCedOBAaBpqPHEycrNIRHoA/wW8AnQFfht3SQwGg6EZtMU2i36fC+++lMYjRsGT7qe6rHGpayrCBqIDdnxtfk8kFjEVgYi4gBJVPQJ8ANSfWNwRMIPFBoOhnVBdFn1RnWNUyZx+YXyEsYmpllQ1QAdZKxALSU1taxEMBsMxRXRzjjOc9WFcXaI3XtXvYGZSE3Ei0TsicpuIDBCR44JH3CVJIGZlscFg6GgEKqNXz1V+2PbQ43GdOeREEVwH3IRlGtpgH01z/9nWOJnbazAYDB0AVyBA8qYNrE/pG7c0nWxMMyRuubUVSUlGGRgMhg6PAEn48aRXMm7XHCA3Luk22iMQkTQR+S8RWWRfnyAizd8vry1o4T7EBoPB0J5I71VFNvGbFu+khnwGqAIm29d5wO+dJC4i54vIThHZJSJ3RLk/RUSOishG+1jgWPImIF26JCJZg8FgSCANzHR0QebA+I57OllHMFRVrxSRqwFUtUKk8U2ARcQNPAqci6U8PhGRV1R1W52oH6pqQnsY4dviGQwGQ8cgejvdk17D3vez6DaknHiNEjjpEVTZLqcVQESGAk7ceZ4O7LK9l1Zh7Wh2SSPPGAwGgwFoaHpqdWkSGgDfkfi513fSI7gLeBMYICJ/A84AZjh4rj8Q7hAjD4i2yeYkEdkE7ANuU9WtdSOIyCxgFkDv3r1ZtWqVg+xr6SnSolm/BoPB0K4Q6DW6pMl1YUM4mTX0tohsACZa2XOLqjpxjh2t3q27LvpTYJCqlonIdOAl4IQoMiwCFoG1ef2UKVMcZF/L9o6xkZrBYDA4wtO1hvTeVTS1LmwIJ7OGXgHOA1ap6msOlQBYPYABYdc5WK3+EPYeyGX2+QrAIyLZDtN3jKSlNx7JYDAYOgjJGfGdDu9kjOAB4DvANhF5QUQuszeraYxPgBNEZIiIJANXYTmtCyEifYIDzyJyui2Pc0fkDjEmIYPBcKygCN59KXgPJsctTSemofeB9+1ZQGcDNwBPAzE3FFXVGhGZC7wFuIGnVXWriMy27z8BXAbMEZEaoAK4SutuHhoHAsbFhMFgOIZQoCQ3lXjZOhxtIGrPGroIuBI4Dfizk+dsc8+KOmFPhJ0/AjziVNjmIqmpaGmH2lTNYDAYGqRahLzK5LhNH3WyZ/FzWLN93sRaF7DK9kraYdCysrYWwWAwGOKGR6G0uHWnjz4D/EhV/QAicoaI/EhVb4qbFIkm/tYmg8FgaFNSU+M3YNzoYLGqvgmMEpF7RWQPlnuJHXGToBVwZWW1tQgGg8EQR5STRhbHLbUGewQiciLWTJ+rsWbyPAeIqk6NW+6tRKCoCMXMHjIYDMcGgpLaK377NMcyDe0APgQuUtVdACIyP245tyJ+AZexDhkMhmMCq1l78JPMVvE19EPgALBSRBaLyDl0wEb18i+XI0YJGAyGY4ySPCfLuZzRoCJQ1eWqeiVwErAKmA/0FpHHReS8uEmQYP746R/bWgSDwWCII4ICger4tcudDBZ7VfVvtqvoHGAjUG9vgfbKd/p/p61FMBgMhjgTX+NMk7buUtXDqvqkqp4dVykSyIf5H1LlbmspDAaDIb6k93WyG4Azjvk9HG897VY8/raWwmAwGOJLRUH8fA0d84oAoDS1rSUwGAyG+FIVR+vQMa8IFqxeQFr8elAGg8HQPqiJn837mFcEAH4zRmAwGI4xJI4u3455RdAjuQeVjnysGgwGQ8ehKil+1fcxrwjKqsv4qq+YzSoNBsMxRVmG6RE4plqr6Vph1IDBYDi26FpqegSOSXYn83WfDugbw2AwGGJwKOYekU3jmFcE/zXhv3CpMQ0ZDIZji54l8UvrmFcEl55wKQd7CDUujDIwGAzHBAokxXGfyGNeEQDs7hvfQjMYDIa2RIAj8dq5nk6iCC5cFzBjBAaD4ZiiuGv80uoUisClYhzPGQyGY4pverWiG+pjgXuudJsBAoPBcEyx+hSjCJrMoe5tLYHBYDDEj+veNgvKmkxBdzOF1GAwHDtkH41fWse+IvjDCaBKUeaxYR3SOueNvVO83vlYKDuD4VjioxHxS+vYVwTeAgD2ZXXMl41W2Ue71gbOnaYRLd/G4tbNpzlpOMk/XijOlGei5XCSl1G8htakI9aNzaJfEVS14ts2549ct+IOXnuT68c7FDZ1zOeOnqcQGd6QgmioAqp7P9Y7RatgJSxMiEyvKQpOgUADcRqS1QlO4zotg3gTLU8nStdJmobYtNdyCspVA/QsjV+6nUYRHOwh7OkTny/Yacst1r3GKjZvMgR32Eyphpqwb+rTofDWt1wcTYMagWQ/fNEPNgyDsi5W2jUCFUngl8j86soWLke0imf9MKsSDxC78i5Oq02vbqUcCDuv8ERW7v6wdKIpJV+SFSdcqYSn11APqO69uvJHC2+sZ0UD9+oqq2iyNCRfYzSmDBpS3A29S2PpO1W0sd4rmmxO0qybVlOVW6zGQGMNj+b+p5tbnziRJVbaScCmwWbWUJPZ3RdO2GedV9RZU+CkZVr33OmPN9oPPVixVbqix1Wsyv9v5whf9LMrYoHPB1kV48l51vvMuiWJVaOF98YIv70mifsuT+LBH7rwdYGVo4VrfpnEytFCRbIVf+sAK5+irrCrL+zsZ/0AAliKJliRBQT2Zllhgw/C22Mh4AIVqHZDtYRVpAIFmdC93FIalR4rDGqVkN9lKaoqNyTXwF/PEf77Ry7ybXOdL6k2ngJHU+1ycsHzZwkBtxUWEOszvKcTCPsUoDADSrtEfi+Hulr38rKsz+I0S7ad/ex3sOPVfa7CU/sOwV6ZAsWpkfEUq7cZ3usB633rKolYCjh4XZFUex7+GwlXNDVE0lBlGC08luKsK1NDyjP8uiE5ooU1NM+lbj51e7MNxa8rU7T0Gsurbni0vJ0o1mhlHCvvWDSWV5Urvt4SErpli4icDzwEuIElqnpPnfti358OlAMzVPXTRMgyeZtVnAEgxW6GOmkFRPtxBgl+D4JVubuAahckh31BBZlQkQyDC2ufEazKNClgVbgDiqxKzu+ClafCkAPQ/7BlzsrtJeQUKvdf5mLrIBcjvglw24sBJm9Ttg6CxRdEarWh++H+H1pxwbq/+hQr/oSdyotnCOd9qjw71cVFa5UNw5TXT3cxdL+lLAYfVCZtUz4+ycXuvlZ6AKtOVXqUKSfnwupRwr4sGLFHOXUPuNRSFj2PCh+NsOY3X/hxgEEHYcXpQr8iq0e2bIr1PbgDwuCDSk6R9dxT5yfx33+u4cR9ljLZOcDK+7YXA0zapvzvlbXyTd6mTPlcOdgd3h1rpT15m3KkKxzoAUcyhLM3Kfk9oN8RSwH99pokfvlCDeN21cr56kRh6H4oSwuQ6YWv+8DZm6z4wT2uR+bC384WRuxRRn8NX/WGvkWQXmX1vgAGHYTuXhCxlamAx1/7fQZckFRjldHRVOhWYXmN7FliK0wFdYErYP0u1g8Dd0DYPAQu+1CpdoPWwNZBkOmFNacIV61SkgKwqxcMPQglqZBSBV/3tn4vAVHO+8z6bYlC32Ir/SNp0KPckvtwOvTw1v5uqtyQ5Ld+w8HferhCc/mtyqK0C2SEbf1anALdfJGVaPC/UPc/FPyv1GBVCBHmQrHeo3t55DN1/5fh/8HgvRqBJLVMpOH/7YCAu04C4XIGqG0ICVbl6sYKcNvxgmnX7ZES9o4Boreo6753eFgwvfB0g+UWDAvmHSxzse+VJcNLkwXXiIoouTYPUW1u56aRhEXcwBfAuUAe8AlwtapuC4szHbgZSxFMAB5S1Qmx0h0/fryuX7/euSB3dWPU4AFcvE7Z3RfmvBqgZ2ltZezC+pOC1ZrsURb54wn+YAT4qg8cf8AKD1YYp+6xIlR5IC8bhu+D/T2gVzF8NhS6eWFwgfVHLOhuhVcnwfNnWhXMyFzYMgiOdBVWnyLMXx5g4aXWzypYCe/uS6hiBxjxTYCh++GVic46dCO+CYTSDSqT8GunXLw20GJZgtzxnJ/NQ+D1092htHsWB+h5VKwFgA2kHUuG3X0JvdfQ/eB3KZeu1oj3bkjWhtINKtBgGje84WfytlrFDHDDG34GHbSUa1AOgKtXBvimd23VdbCH4Hcpo76GgEtBrd7d6pOtOAMLlEGH4N7La+W9aK3y6kQJ5TXimwC3/SPA9hw4cR8snyxculpDnwsvtRR8sGyD7zX4oPLDD5UuNfDeaEsZbh4CP1qpHEmHt8e7OH1HABUYchDKk+HlyUKfw8rZm2DT8ZBRDn2OQFUSdPXBByPhrC2weRCctstuCAn879Uubnw1QHZpbcX7r7Hw3Y3WxWdDLSWbXG3d23cc/OJnSYz4JsCvXgiEeojplVZjye+CXX2s/xZEKo+/nS3s6S1M3qackqskV1u90q2D4NzPLIU96CBkl9Yq2uKuwre3KCJQmmKluac3jN8FfzlHGPGNMma3pcQ/HwIjvrFkFaz0TtlrfWc9ypThudC1yuopJ/nB54Eu1dazviTrXpCQ4rFreheW8kkOWErAjdWYELv+OdIVijKs966xe5wSsJ57eywsP1d4/7ptOEVENqjq+Kj3EqgIJgF3qeo0+/rXAKr6v2FxngRWqeqz9vVOYIqq7m8o3eYogp/06cXGlC4gwh3P+elzROl3xDK1HOwhDCxQ0nywY6AwebtS7YKSNOh32PpSXNS21AIuZfRXsOpU4WAPCf3pgxXG8smCOyChSumL/nByLvVa9KtPqX0+HhVrLOJZgbdnEvGeiSy75qQdfCaocMIVXDC8sWdj5deUOOEy/H5pDSVp8PoEV+i/sL+H9f95dqqLXz0f4LmzLIV35QfKvZdbaYUr2vD0Ll4boPcRqxd6pKv13Le3KlsHWr2+4LOrT5F6vWKIbGiEK8NRX1ueBm54w+o6BJ+9eG0Av8vqrUJt77iurN/Zojx3lvD66e5Qgyr8P/+r5wNsGQyvn+4K9V7VZdU1RzIsBZRcY1Xq74yFQ91d+F3KFR8oHr/V63x2am0ZlqRCz6NQk2T19K13C+BW2Hd+GdMe3Bv1u45GWymCy4DzVXWmff0TYIKqzg2L8xpwj6p+ZF+/C/xKVdfXSWsWMAugd+/e45YtW+ZYjrNWXoIIjBqUAyKMyFXmLw/w9mmWiSS8VRz8wUBty3LyNqVHqXLiPmK2LBv6A9Vt0QXDm1SRqFq2h/DvKnhd9zOy4ByXU7sj+E6JTB8az6MpcjQW18n9oEyNfeex0owWJ/z54LkTmRpKN8Z5sPcd7TcPUXq4ewIM3a+8Msld/31t6v2/VBmRq9b/aIJEL7No79VQWUT5L128NsDuflJf1gPWfzdCJjutkEwTa5XZ6pOFrYNre3TBnuLi812hvG54wxrtWjw9KSTniFzlorUBijIsa0EojT0Brvy8msld/Gyf9VRj31qIqVOntokiuByYVkcRnK6qN4fFeR343zqK4HZV3dBQuk3tEQy+43X2pPyIPLeb2wJ9mf9S4yaSVm9BN1SRJ4KGFEa0CqI5aTRUUbREvmhxmppue6I1v2snOC1vp/Gbkjfgxo0/NH8sQTT2e3ZIkiRRo3WH6luYfwwZXLgI1BkK75XWi1e//yppnrQmiRCrR5BIu0AeMCDsOgfY14w4LSLFbRVwjt/PVbvLWfh9F1sHWmFbBwoLv+9i6D61vij7eGWCWHGCX54qWweKpQRUox92vGYdwWfr0EWVATU1PHSwkKHV1SzPP8DmPXtZnn+AwdUByvf+FH9lH+4+UB5xf/OevVxfbG1flBSoM7Ug2g8yPCxcpoaIlUa08miA5LqyhaXzrfIKXA09GyVdERdZqdmkJqVZf6qwIy1gxXUBXWusCueOoiP8oML6QwvQrcYfVdbrvTX0Se8bkV5KUiqIkJGcSZ/0vvRJ78uAzIE8NPVhsj096ZPel66eDBBBxEW3Lt1Jcnv46Yhrav/wInT1ZEQ8v/ySl7jj9F+DCNePmsnQHsNYfslLbJ6xhYemPozHncwdp/+aAZkDQ+l/d+C5uCUJERcucdMnvS/Xj5ppyefJIMmVxENTHuKG7BvwuDw8NOUhHpryEEmSRJ+0PtzxrTtIcnno1qU7LnHjEne9c7e4a+NKEn3S+/LQ1IfJSs0GEX5wwg9Jcnnok96XO07/tZVecjfc4g59ZqVk0SetD1kpWbhwIQguXPRw9+DFi19k+cXLGdp9KMsvXs7mazaz/OLlDMgYQFZKVkQ6XZOsBTTXj7yePml9SHIlcf3I6/G4PNzxrTtCadzxrTvs79bKJys1O1TGwbJ8aOrDLL/kJYZ2H8pDUx6K+PzPvv/J8ouXM6TbEHqm9kQQ+qX3468X/DUUb0DGAPqk9WFAxgCWX7yc5RcvJyslC4AMT0ZEubnEHfrNuMSN25XE9aNmhuQIf+cBGQNCcvzj4n+w+ZrNoffJdGXy5HefbLISaIxE9giSsAaLzwHysQaLf6SqW8PiXAjMpXaw+GFVPT1Wuk0eIwACd3bHZY/A/KF7N/7SPY6bfTYBjbYrgrqoLj0FT+YWzux5BY9d+NuE5b/ryC5u++A27j/zfob1GFYvDGDOv+ZQ6CvkgTMf4OxBZ7Nq1SqmTJniKI15Y+Zx/wYrndvG3RY6f3jqw6G4TZUvWpx5K+dRUVHB4umLHaXbmtQtr7o4ece2kKut6AhytdV31phcTSVWjwBVTdiBVcF/AewGfmOHzQZm2+cCPGrf3wyMbyzNcePGaXNZuXKlqqp+efhLveSlS/TLw1/GjP/unnd17F/G6rt73o35jNP0GpOrPdJeZTNyNQ0jV9M4FuUC1msD9WpC1xGo6gpgRZ2wJ8LOFbgpkTJEY1iPYbx0yUuNxjt70Nl8+pPaZQ0NPeM0PYPBYGiPHDtzBw0Gg8HQLIwiMBgMhk6OUQQGg8HQyTGKwGAwGDo5CZs+mihE5BDwTTMfzwYK4yhOvGivckH7lc3I1TSMXE3jWJRrkKr2jHajwymCliAi67WhebRtSHuVC9qvbEaupmHkahqdTS5jGjIYDIZOjlEEBoPB0MnpbIpgUVsL0ADtVS5ov7IZuZqGkatpdCq5OtUYgcFgMBjq09l6BAaDwWCog1EEBoPB0MnpNIpARM4XkZ0isktE7milPPeIyGYR2Sgi6+2w40TkHRH50v7sERb/17Z8O0VkWlj4ODudXSLysEjTtu4SkadFpEBEtoSFxU0OEekiIs/Z4etEZHAL5LpLRPLtMtto72vdanKJyAARWSki20Vkq4jc0h7KK4ZcbV1eKSLysYhssuX6XXsor0Zka9Mys59zi8hnYu3S2Pbl1ZBb0mPpwNoXejdwPJAMbAJOaYV89wDZdcLuA+6wz+8A7rXPT7Hl6gIMseV12/c+BiZhue1+A7igiXKcCZwGbEmEHMCNwBP2+VXAcy2Q6y7gtihxW0UuoC9wmn2egeVG/ZS2Lq8YcrV1eQnQ1T73AOuAiW1dXo3I1qZlZsf9OfB34LX28H9ss8q5NQ+7sN4Ku/418OtWyHcP9RXBTqCvfd4X2BlNJuAtW+6+wI6w8KuBJ5shy2AiK9y4yRGMY58nYa18lGbK1dCftFXlCkvvZeDc9lJeUeRqN+UFpAGfYm0y1d7KK1y2Ni0zrJ0Y3wXOplYRtGl5dRbTUH9gb9h1nh2WaBR4W0Q2iMgsO6y3qu4HsD97NSJjf/u8bnhLiaccoWdUtQY4CmS1QLa5IvK5WKajYBe51eWyu9RjsVqS7aa86sgFbVxetpljI1AAvKOq7aa8GpAN2rbM/gjcDhGbEbdpeXUWRRDNpt4a82bPUNXTgAuAm0TkzBhxG5KxtWVvjhzxlPFxYCgwBtgPPNAWcolIV+AfwK2qWhIrahvL1eblpap+VR2D1dI9XURGxnqF1pIrhmxtVmYi8j2gQFU3OJG/NWSCzqMI8oABYdc5wL5EZ6qq++zPAmA5cDpwUET6AtifBY3ImGef1w1vKfGUI/SMWHtVdwMON0coVT1o/3kDwGKsMmtVuUTEg1XZ/k1V/2kHt3l5RZOrPZRXEFUtBlYB59MOyqsh2dq4zM4ALhaRPcAy4GwR+T/auLw6iyL4BDhBRIaISDLWAMoricxQRNJFJCN4DpwHbLHzvcaOdg2WrRc7/Cp7xH8IcALwsd1NLBWRifasgJ+GPdMS4ilHeFqXAe+pbaBsKsE/g82lWGXWanLZaTwFbFfVB8NutWl5NSRXOyivniLS3T5PBb4L7KAd/L4akq0ty0xVf62qOao6GKseek9Vf9zm5dWUAZeOfADTsWZa7AZ+0wr5HY812r8J2BrME8tW9y7wpf15XNgzv7Hl20nYzCBgPNaPdTfwCE0fKHsWqwtcjdVauD6ecgApwAvALqyZDMe3QK6/ApuBz+0fdN/WlAv4NlY3+nNgo31Mb+vyiiFXW5fXqcBndv5bgAXx/p234PfVkGxtWmZhaU6hdrC4TcvLuJgwGAyGTk5nMQ0ZDAaDoQGMIjAYDIZOjlEEBoPB0MkxisBgMBg6OUYRGAwGQyfHKAKDwUZE/FLrkXKjxNFLrYgMljAvqwZDeyKprQUwGNoRFWq5IzAYOhWmR2AwNIJY+0rcK5Zv+49FZJgdPkhE3rWdl70rIgPt8N4islwsP/ibRGSynZRbRBaL5Rv/bXu1KyIyT0S22eksa6PXNHRijCIwGGpJrWMaujLsXomqno61gvOPdtgjwF9U9VTgb8DDdvjDwPuqOhprv4WtdvgJwKOqOgIoBn5oh98BjLXTmZ2YVzMYGsasLDYYbESkTFW7RgnfA5ytql/Zjt8OqGqWiBRiuSeotsP3q2q2iBwCclS1MiyNwVhukE+wr38FeFT19yLyJlAGvAS8pKplCX5VgyEC0yMwGJyhDZw3FCcalWHnfmrH6C4EHgXGARtsj5EGQ6thFIHB4Iwrwz7X2OersTxIAvwH8JF9/i4wB0Ibo2Q2lKiIuIABqroSa7OS7kC9XonBkEhMy8NgqCVVrN2sgrypqsEppF1EZB1W4+lqO2we8LSI/BI4BFxrh98CLBKR67Fa/nOwvKxGww38n4h0w9pQZKFavvMNhlbDjBEYDI1gjxGMV9XCtpbFYEgExjRkMBgMnRzTIzAYDIZOjukRGAwGQyfHKAKDwWDo5BhFYDAYDJ0cowgMBoOhk2MUgcFgMHRy/j9T4gu1i4dqHgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.78%\n"
     ]
    }
   ],
   "source": [
    "# run the four layer model\n",
    "nn = FourLayerPerceptron(n_hidden1=15, n_hidden2=15, n_hidden3=15, l2_C=0.0, epochs=500, eta=0.001, random_state=1)\n",
    "nn.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "nn.plot_gradient_magnitudes()\n",
    "\n",
    "# Check the accuracy\n",
    "y_pred = nn.predict(X_test)\n",
    "y_pred = convert_data(y_pred)\n",
    "print('Accuracy: %.2f%%' % (100 * accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1 points] Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveLayerPerceptron(FourLayerPerceptron):\n",
    "    def __init__(self, n_hidden1=30, n_hidden2=30, n_hidden3=30, n_hidden4=30, epochs=500, eta=0.001, random_state=None, l2_C=0.0):\n",
    "        super().__init__(n_hidden1, n_hidden2, n_hidden3, epochs, eta, random_state, l2_C)\n",
    "        self.n_hidden4 = n_hidden4\n",
    "\n",
    "    # update feedforward method to include 5 layers\n",
    "    def _feedforward(self, X, W1, W2, W3, W4, W5):\n",
    "        \"\"\"Compute feedforward step for a five-layer perceptron.\"\"\"\n",
    "        A1 = self._add_bias_unit(X.T, how='row')\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        \n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        A3 = self._add_bias_unit(A3, how='row')\n",
    "        \n",
    "        Z3 = W3 @ A3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        A4 = self._add_bias_unit(A4, how='row')\n",
    "        \n",
    "        Z4 = W4 @ A4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        A5 = self._add_bias_unit(A5, how='row')\n",
    "        \n",
    "        Z5 = W5 @ A5\n",
    "        A6 = self._sigmoid(Z5)\n",
    "\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6\n",
    "\n",
    "\n",
    "    # Add gradient calculation for the fifth layer\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5, Y_enc, W1, W2, W3, W4, W5):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        V5 = -2 * (Y_enc - A6) * A6 * (1 - A6)\n",
    "        V4 = A5 * (1 - A5) * (W5.T @ V5)\n",
    "        V3 = A4 * (1 - A4) * (W4.T @ V4[1:, :])\n",
    "        V2 = A3 * (1 - A3) * (W3.T @ V3[1:, :])\n",
    "        V1 = A2 * (1 - A2) * (W2.T @ V2[1:, :])\n",
    "\n",
    "        grad5 = V5 @ A5.T\n",
    "        grad4 = V4[1:, :] @ A4.T\n",
    "        grad3 = V3[1:, :] @ A3.T\n",
    "        grad2 = V2[1:, :] @ A2.T\n",
    "        grad1 = V1[1:, :] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        grad3[:, 1:] += W3[:, 1:] * self.l2_C\n",
    "        grad4[:, 1:] += W4[:, 1:] * self.l2_C\n",
    "        grad5[:, 1:] += W5[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2, grad3, grad4, grad5\n",
    "\n",
    "    # Add the fifth layer to the weight initialization\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using the Glorot initialization.\"\"\"\n",
    "        glorot = lambda n_in, n_out: np.sqrt(6 / (n_in + n_out))\n",
    "\n",
    "        W1_shape = (self.n_hidden1, self.n_features_ + 1)\n",
    "        W1 = np.random.uniform(-glorot(self.n_features_, self.n_hidden1), glorot(self.n_features_, self.n_hidden1), size=W1_shape)\n",
    "\n",
    "        W2_shape = (self.n_hidden2, self.n_hidden1 + 1)\n",
    "        W2 = np.random.uniform(-glorot(self.n_hidden1, self.n_hidden2), glorot(self.n_hidden1, self.n_hidden2), size=W2_shape)\n",
    "\n",
    "        W3_shape = (self.n_hidden3, self.n_hidden2 + 1)\n",
    "        W3 = np.random.uniform(-glorot(self.n_hidden2, self.n_hidden3), glorot(self.n_hidden2, self.n_hidden3), size=W3_shape)\n",
    "\n",
    "        W4_shape = (self.n_hidden4, self.n_hidden3 + 1)\n",
    "        W4 = np.random.uniform(-glorot(self.n_hidden3, self.n_hidden4), glorot(self.n_hidden3, self.n_hidden4), size=W4_shape)\n",
    "\n",
    "        W5_shape = (self.n_output_, self.n_hidden4 + 1)\n",
    "        W5 = np.random.uniform(-glorot(self.n_hidden4, self.n_output_), glorot(self.n_hidden4, self.n_output_), size=W5_shape)\n",
    "\n",
    "        return W1, W2, W3, W4, W5\n",
    "\n",
    "\n",
    "    # Add the fifth layer to the prediction function\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, _, _, A6 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.W5)\n",
    "        y_pred = np.argmax(A6, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, batch_size=32):\n",
    "        \"\"\"Learn weights from training data using mini-batch gradient descent.\"\"\"\n",
    "\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': [], 'W3': [], 'W4': [], 'W5': []}  # Added line to store gradient magnitudes\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # shuffle the data before each epoch\n",
    "            idx = np.random.permutation(X_data.shape[0])\n",
    "            X_data = X_data[idx]\n",
    "            Y_enc = Y_enc[:, idx]\n",
    "\n",
    "            mini_batch_indices = np.array_split(np.arange(X_data.shape[0]), X_data.shape[0] // batch_size)\n",
    "\n",
    "            for batch_indices in mini_batch_indices:\n",
    "                X_mini_batch = X_data[batch_indices]\n",
    "                Y_enc_mini_batch = Y_enc[:, batch_indices]\n",
    "\n",
    "                # feedforward all instances in the mini-batch\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6 =self._feedforward(X_mini_batch, self.W1, self.W2, self.W3, self.W4,self.W5)\n",
    "\n",
    "                cost = self._cost(A6, Y_enc_mini_batch, self.W1, self.W2, self.W3, self.W4, self.W5)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2, grad3, grad4, grad5 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5, A6=A6,\n",
    "                                                                        Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5,\n",
    "                                                                        Y_enc=Y_enc_mini_batch,\n",
    "                                                                        W1=self.W1, W2=self.W2, W3=self.W3, W4=self.W4, W5=self.W5)\n",
    "\n",
    "                # Save average gradient magnitudes\n",
    "                self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "                self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "                self.grad_mag_['W3'].append(np.mean(np.abs(grad3)))\n",
    "                self.grad_mag_['W4'].append(np.mean(np.abs(grad4)))\n",
    "                self.grad_mag_['W5'].append(np.mean(np.abs(grad5)))\n",
    "\n",
    "                self.W1 -= self.eta * grad1\n",
    "                self.W2 -= self.eta * grad2\n",
    "                self.W3 -= self.eta * grad3\n",
    "                self.W4 -= self.eta * grad4\n",
    "                self.W5 -= self.eta * grad5\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _cost(self, A6, Y_enc, W1, W2, W3, W4, W5):\n",
    "        \"\"\"Compute the cost function.\"\"\"\n",
    "        cost = np.mean((Y_enc - A6)**2)\n",
    "        L2_term = (self.l2_C * (np.sum(W1[:, 1:] ** 2) +\n",
    "                                np.sum(W2[:, 1:] ** 2) +\n",
    "                                np.sum(W3[:, 1:] ** 2) +\n",
    "                                np.sum(W4[:, 1:] ** 2) +\n",
    "                                np.sum(W5[:, 1:] ** 2)\n",
    "                                ))\n",
    "        return cost + L2_term\n",
    "\n",
    "    def plot_gradient_magnitudes(self):\n",
    "        \"\"\"Plot the average gradient magnitudes for each layer.\"\"\"\n",
    "        plt.plot(self.grad_mag_['W1'], label='W1', linestyle='-', marker='o')\n",
    "        plt.plot(self.grad_mag_['W2'], label='W2', linestyle='-', marker='s')\n",
    "        plt.plot(self.grad_mag_['W3'], label='W3', linestyle='-', marker='d')\n",
    "        plt.plot(self.grad_mag_['W4'], label='W4', linestyle='-', marker='x')\n",
    "        plt.plot(self.grad_mag_['W5'], label='W5', linestyle='-', marker='v')\n",
    "\n",
    "        plt.ylabel('Average Gradient Magnitude')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.86%\n"
     ]
    }
   ],
   "source": [
    "# run the five layer model\n",
    "num = 16\n",
    "nn = FiveLayerPerceptron(n_hidden1=num, n_hidden2=num, n_hidden3=num, n_hidden4=num, l2_C=0.0, epochs=500, eta=0.001, random_state=1)\n",
    "nn.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "nn.plot_gradient_magnitudes()\n",
    "\n",
    "# Check the accuracy\n",
    "y_pred = nn.predict(X_test)\n",
    "y_pred = convert_data(y_pred)\n",
    "print('Accuracy: %.2f%%' % (100 * accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (such as AdaGrad, RMSProps, or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveLayerPerceptronRMSProp(FiveLayerPerceptron):\n",
    "    def __init__(self, n_hidden1=30, n_hidden2=30, n_hidden3=30, n_hidden4=30, epochs=500, eta=0.001, random_state=None, l2_C=0.0, decay_rate=0.9, eps=1e-8):\n",
    "        super().__init__(n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_hidden3=n_hidden3, n_hidden4=n_hidden4, epochs=epochs, eta=eta, random_state=random_state, l2_C=l2_C)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.eps = eps\n",
    "    \n",
    "    def _update_weights(self, grad1, grad2, grad3, grad4, grad5):\n",
    "        # Update the weights using RMSProp\n",
    "        self.sq_grad1 = self.decay_rate * self.sq_grad1 + (1 - self.decay_rate) * grad1**2\n",
    "        self.sq_grad2 = self.decay_rate * self.sq_grad2 + (1 - self.decay_rate) * grad2**2\n",
    "        self.sq_grad3 = self.decay_rate * self.sq_grad3 + (1 - self.decay_rate) * grad3**2\n",
    "        self.sq_grad4 = self.decay_rate * self.sq_grad4 + (1 - self.decay_rate) * grad4**2\n",
    "        self.sq_grad5 = self.decay_rate * self.sq_grad5 + (1 - self.decay_rate) * grad5**2\n",
    "\n",
    "        self.W1 -= (self.eta / np.sqrt(self.sq_grad1 + self.eps)) * grad1\n",
    "        self.W2 -= (self.eta / np.sqrt(self.sq_grad2 + self.eps)) * grad2\n",
    "        self.W3 -= (self.eta / np.sqrt(self.sq_grad3 + self.eps)) * grad3\n",
    "        self.W4 -= (self.eta / np.sqrt(self.sq_grad4 + self.eps)) * grad4\n",
    "        self.W5 -= (self.eta / np.sqrt(self.sq_grad5 + self.eps)) * grad5\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5 = self._initialize_weights()\n",
    "\n",
    "        # RMSProp variables\n",
    "        self.sq_grad1 = np.zeros_like(self.W1)\n",
    "        self.sq_grad2 = np.zeros_like(self.W2)\n",
    "        self.sq_grad3 = np.zeros_like(self.W3)\n",
    "        self.sq_grad4 = np.zeros_like(self.W4)\n",
    "        self.sq_grad5 = np.zeros_like(self.W5)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.grad_mag_ = {'W1': [], 'W2': [], 'W3': [], 'W4': [], 'W5': []}\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # shuffle data\n",
    "            idx = self._shuffle_data(y_data)\n",
    "            X_data, Y_enc = X_data[idx], Y_enc[:, idx]\n",
    "\n",
    "            # iterate over mini-batches\n",
    "            mini = np.array_split(range(X_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3, z4, a4, z5, a5 = self._feedforward(X_data[idx],\n",
    "                                                                        self.W1,\n",
    "                                                                        self.W2,\n",
    "                                                                        self.W3,\n",
    "                                                                        self.W4,\n",
    "                                                                        self.W5)\n",
    "\n",
    "                cost = self._cost(Y_enc=Y_enc[:, idx], output=a5)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradients via backpropagation\n",
    "                grad1, grad2, grad3, grad4, grad5 = self._get_gradient(a1=a1,\n",
    "                                                                    a2=a2,\n",
    "                                                                    a3=a3,\n",
    "                                                                    a4=a4,\n",
    "                                                                    a5=a5,\n",
    "                                                                    z2=z2,\n",
    "                                                                    z3=z3,\n",
    "                                                                    z4=z4,\n",
    "                                                                    z5=z5,\n",
    "                                                                    Y_enc=Y_enc[:, idx],\n",
    "                                                                    W1=self.W1,\n",
    "                                                                    W2=self.W2,\n",
    "                                                                    W3=self.W3,\n",
    "                                                                    W4=self.W4,\n",
    "                                                                    W5=self.W5)\n",
    "\n",
    "                # update weights using RMSProp\n",
    "                self._update_weights(grad1=grad1,\n",
    "                                    grad2=grad2,\n",
    "                                    grad3=grad3,\n",
    "                                    grad4=grad4,\n",
    "                                    grad5=grad5)\n",
    "\n",
    "                # measure gradient magnitudes\n",
    "                self.grad_mag_['W1'].append(np.mean(np.abs(grad1)))\n",
    "                self.grad_mag_['W2'].append(np.mean(np.abs(grad2)))\n",
    "                self.grad_mag_['W3'].append(np.mean(np.abs(grad3)))\n",
    "                self.grad_mag_['W4'].append(np.mean(np.abs(grad4)))\n",
    "                self.grad_mag_['W5'].append(np.mean(np.abs(grad5)))\n",
    "\n",
    "            # print progress\n",
    "            if print_progress:\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f'\\rEpoch: {i+1}/{self.epochs} | Cost: {cost:.4f}', end='')\n",
    "\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (1 points total)\n",
    "5000 level student: You have free reign to provide additional analyses.\n",
    "One idea (required for 7000 level students):  Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4045ce629ca2404df8e09a4bb72649c60795682d1cc7b69b21add64452ef6fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
